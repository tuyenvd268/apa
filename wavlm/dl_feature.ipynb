{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from wavlm import WavLM, WavLMConfig\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = \"/data/codes/prep_ps_pykaldi/pretrained/wavlm-base+.pt\"\n",
    "checkpoint = torch.load(pretrained_path)\n",
    "\n",
    "config = WavLMConfig(checkpoint['cfg'])\n",
    "model = WavLM(config).cuda()\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_path = \"/data/codes/prep_ps_pykaldi/demo/wav/test.wav\"\n",
    "path = \"/data/codes/prep_ps_pykaldi/inputs.json\"\n",
    "\n",
    "inputs = json.load(open(path, \"r\", encoding=\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(alignment, features):\n",
    "    index = 0\n",
    "    phonemes = []\n",
    "    indices = -1 * torch.ones(alignment[-1][1] + alignment[-1][2])\n",
    "    for phoneme, start_frame, duration in alignment:\n",
    "        end_frame = start_frame + duration\n",
    "        indices[start_frame:end_frame] = index\n",
    "        phonemes.append(phoneme)\n",
    "        index += 1\n",
    "\n",
    "    indices[indices==-1] = indices.max() + 1\n",
    "\n",
    "    indices = torch.nn.functional.one_hot(indices.long(), num_classes=int(indices.max().item())+1).cuda()\n",
    "    indices = indices / indices.sum(0, keepdim=True)\n",
    "    \n",
    "    if features.shape[0] != indices.shape[0]:\n",
    "        features = features[0:indices.shape[0]]\n",
    "    features = torch.matmul(indices.transpose(0, 1), features)\n",
    "\n",
    "    return features, phonemes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment = inputs[\"alignments\"]\n",
    "waveform, _ = librosa.load(wav_path, sr=16000)\n",
    "\n",
    "waveform = torch.from_numpy(waveform).unsqueeze(0).cuda()\n",
    "with torch.no_grad():\n",
    "    features = model.extract_features(waveform)[0]\n",
    "\n",
    "index = torch.arange(features.shape[1]).unsqueeze(-1).cuda()\n",
    "expanded_index = index.expand((-1, 2)).flatten()\n",
    "features = features[0][expanded_index]\n",
    "features, phonemes = extract_feature(alignment, features)\n",
    "features = features[0:len(phonemes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>alignment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3666010</td>\n",
       "      <td>[[\"SIL\", 0, 265], [\"K_B\", 265, 15], [\"AO1_I\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3666014</td>\n",
       "      <td>[[\"SIL\", 0, 17], [\"K_B\", 17, 14], [\"AE1_I\", 31...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                          alignment\n",
       "0  3666010  [[\"SIL\", 0, 265], [\"K_B\", 265, 15], [\"AO1_I\", ...\n",
       "1  3666014  [[\"SIL\", 0, 17], [\"K_B\", 17, 14], [\"AE1_I\", 31..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_path = \"/data/codes/prep_ps_pykaldi/exp/sm/test/merged_align.out\"\n",
    "align_df = pd.read_csv(align_path, names=[\"id\", \"alignment\"], sep=\"\\t\")\n",
    "align_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(alignment, features):\n",
    "    index = 0\n",
    "    phonemes = []\n",
    "    indices = -1 * torch.ones(alignment[-1][1] + alignment[-1][2])\n",
    "    for phoneme, start_frame, duration in alignment:\n",
    "        if phoneme == \"SIL\":\n",
    "            continue\n",
    "        end_frame = start_frame + duration\n",
    "        indices[start_frame:end_frame] = index\n",
    "        phonemes.append(phoneme)\n",
    "        index += 1\n",
    "\n",
    "    indices[indices==-1] = indices.max() + 1\n",
    "\n",
    "    indices = torch.nn.functional.one_hot(indices.long(), num_classes=int(indices.max().item())+1).cuda()\n",
    "    indices = indices / indices.sum(0, keepdim=True)\n",
    "    \n",
    "    if features.shape[0] != indices.shape[0]:\n",
    "        features = features[0:indices.shape[0]]\n",
    "    features = torch.matmul(indices.transpose(0, 1), features)\n",
    "\n",
    "    return features, phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_dir = \"/data/codes/prep_ps_pykaldi/prep_data/wav\"\n",
    "\n",
    "wavlm_features = []\n",
    "for index in tqdm(align_df.index):\n",
    "    wav_id = align_df[\"id\"][index]\n",
    "    alignment = align_df[\"alignment\"][index]\n",
    "\n",
    "    alignment = json.loads(alignment)\n",
    "    wav, sr = librosa.load(f'{wav_dir}/{wav_id}.wav', sr=16000)\n",
    "\n",
    "    # input_values = torch.from_numpy(wav).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        features = processor(wav, return_tensors=\"pt\", sampling_rate=16000)\n",
    "        features = model(features[\"input_values\"].cuda()).last_hidden_state\n",
    "\n",
    "    index = torch.arange(features.shape[1]).unsqueeze(-1).cuda()\n",
    "    expanded_index = index.expand((-1, 2)).flatten()\n",
    "    features = features[0][expanded_index]\n",
    "\n",
    "    features, phonemes = extract_feature(alignment, features)\n",
    "\n",
    "    sample = {\n",
    "        \"id\": wav_id,\n",
    "        \"phoneme\": phonemes,\n",
    "        \"features\": features[0:len(phonemes)].cpu().numpy()\n",
    "    }\n",
    "    wavlm_features.append(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
