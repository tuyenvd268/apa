{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /data/codes/prep_ps_pykaldi/\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 32\n",
    "\n",
    "metadata_path = \"/data/codes/prep_ps_pykaldi/prep_data/jsonl_v1/info_qt_10_trainset.jsonl\"\n",
    "align_path = \"/data/codes/prep_ps_pykaldi/exp/sm/train_new/merged_align.out\"\n",
    "gop_path = '/data/codes/prep_ps_pykaldi/exp/sm/train_new/merged_gop.pkl'\n",
    "out_dir = \"/data/codes/prep_ps_pykaldi/exp/sm/train_new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [json.loads(line.strip()) for line in lines]\n",
    "    \n",
    "    lines = pd.DataFrame(lines)\n",
    "    return lines\n",
    "\n",
    "metadata = load_jsonl(metadata_path)\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(gop_path, 'rb') as f:\n",
    "    gop_features = pickle.load(f)\n",
    "    \n",
    "metadata = metadata[metadata.id.isin(gop_features)]\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gop_feature(id):\n",
    "    sample = gop_features[str(id)]\n",
    "    features = [\n",
    "        np.array(feature) for feature, phoneme in zip(sample[\"gopt\"], sample[\"phones\"][0])\n",
    "        if phoneme != \"SIL\"\n",
    "    ]\n",
    "    return np.stack(features)\n",
    "\n",
    "def extract_phonemes(id):\n",
    "    sample = gop_features[str(id)]\n",
    "    phonemes = [\n",
    "        re.sub(\"\\d\", \"\",phoneme.split(\"_\")[0]) for phoneme in sample[\"phones\"][0]\n",
    "        if phoneme != \"SIL\"\n",
    "    ]\n",
    "    return phonemes\n",
    "\n",
    "metadata[\"features\"] = metadata.id.apply(lambda x: extract_gop_feature(x))\n",
    "metadata[\"kaldi_phoneme\"] = metadata.id.apply(lambda x: extract_phonemes(x))\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_df = pd.read_csv(align_path, names=[\"id\", \"alignment\"], sep=\"\\t\")\n",
    "\n",
    "def extract_duration(alignment):\n",
    "    alignment = json.loads(alignment)\n",
    "    durations = []\n",
    "    \n",
    "    for phoneme, start, duration in alignment:\n",
    "        if phoneme == \"SIL\":\n",
    "            continue\n",
    "        durations.append(round(duration * 0.02, 4))\n",
    "\n",
    "    return durations\n",
    "\n",
    "def extract_phonemes(alignment):\n",
    "    alignment = json.loads(alignment)\n",
    "    phonemes = []\n",
    "    \n",
    "    for phoneme, start, duration in alignment:\n",
    "        if phoneme == \"SIL\":\n",
    "            continue\n",
    "        phonemes.append(phoneme.split(\"_\")[0])\n",
    "\n",
    "    return phonemes\n",
    "\n",
    "align_df[\"durations\"] = align_df[\"alignment\"].apply(lambda x: extract_duration(x))\n",
    "align_df[\"phonemes\"] = align_df[\"alignment\"].apply(lambda x: extract_phonemes(x))\n",
    "align_df[\"id\"] = align_df[\"id\"].apply(str)\n",
    "align_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.merge(metadata, align_df[[\"id\", \"durations\", \"alignment\"]], how=\"left\", on=\"id\")\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"length\"] = metadata[\"arpas\"].apply(len)\n",
    "metadata[\"length\"].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sentence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = []\n",
    "\n",
    "for index in metadata.index:\n",
    "    sentence_score = metadata[\"utterance_scores\"][index].copy()\n",
    "\n",
    "    sentence_scores.append(sentence_score)\n",
    "\n",
    "sentence_scores = torch.tensor(sentence_scores)\n",
    "sentence_scores = sentence_scores.numpy()\n",
    "print(sentence_scores.shape)\n",
    "np.save(f'{out_dir}/sentence_scores.npy', sentence_scores)\n",
    "sentence_scores = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract word scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_scores = []\n",
    "\n",
    "for index in metadata.index:\n",
    "    word_score = metadata[\"word_scores\"][index].copy()\n",
    "    word_id = metadata[\"word_ids\"][index].copy()\n",
    "\n",
    "    word_score_in_phone_levels = []\n",
    "    for wid in word_id:\n",
    "        word_score_in_phone_levels.append(word_score[wid])\n",
    "\n",
    "    padding = [-1,]*(MAX_LENGTH-len(word_score_in_phone_levels))\n",
    "    word_score_in_phone_levels = word_score_in_phone_levels + padding\n",
    "    word_score_in_phone_levels = torch.tensor(word_score_in_phone_levels)\n",
    "    word_scores.append(word_score_in_phone_levels)\n",
    "\n",
    "word_scores = torch.stack(word_scores, dim=0)\n",
    "word_scores = word_scores.numpy()\n",
    "print(word_scores.shape)\n",
    "np.save(f'{out_dir}/word_scores.npy', word_scores)\n",
    "word_scores = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract word ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = []\n",
    "\n",
    "for index in metadata.index:\n",
    "    word_id = metadata[\"word_ids\"][index].copy()\n",
    "\n",
    "    padding = [-1,]*(MAX_LENGTH-len(word_id))\n",
    "    word_id = word_id + padding\n",
    "    word_id = torch.tensor(word_id)\n",
    "    word_ids.append(word_id)\n",
    "\n",
    "word_ids = torch.stack(word_ids, dim=0)\n",
    "word_ids = word_ids.numpy()\n",
    "print(word_ids.shape)\n",
    "np.save(f'{out_dir}/word_ids.npy', word_ids)\n",
    "word_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract gop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gops = []\n",
    "\n",
    "for index in metadata.index:\n",
    "    gop = metadata[\"features\"][index].copy()\n",
    "\n",
    "    padding = [[0,]*len(gop[0]),]*(MAX_LENGTH-len(gop))\n",
    "    gop = gop.tolist() + padding\n",
    "    gop = torch.tensor(gop)\n",
    "    gops.append(gop)\n",
    "\n",
    "gops = torch.stack(gops, dim=0)\n",
    "gops = gops.numpy()\n",
    "print(gops.shape)\n",
    "np.save(f'{out_dir}/gop.npy', gops)\n",
    "gops = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = []\n",
    "\n",
    "for index in metadata.index:\n",
    "    duration = metadata[\"durations\"][index].copy()\n",
    "\n",
    "    padding = [0, ]*(MAX_LENGTH-len(duration))\n",
    "\n",
    "    duration += padding\n",
    "    duration = torch.tensor(duration)\n",
    "    durations.append(duration)\n",
    "\n",
    "durations = torch.stack(durations, dim=0)\n",
    "durations = durations.numpy()\n",
    "np.save(f'{out_dir}/duration.npy', durations)\n",
    "durations = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract phone scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_scores = []\n",
    "\n",
    "for index in metadata.index:\n",
    "    phone_score = metadata[\"phone_scores\"][index].copy()\n",
    "\n",
    "    padding = [-1, ]*(MAX_LENGTH-len(phone_score))\n",
    "\n",
    "    phone_score += padding\n",
    "    phone_score = torch.tensor(phone_score)\n",
    "    phone_scores.append(phone_score)\n",
    "\n",
    "phone_scores = torch.stack(phone_scores, dim=0)\n",
    "phone_scores = phone_scores.numpy()\n",
    "np.save(f'{out_dir}/phone_scores.npy', phone_scores)\n",
    "phone_scores = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract phone ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_dict_path =  \"/data/codes/prep_ps_pykaldi/resources/phone_dict.json\"\n",
    "with open(phone_dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    phone_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_ids = []\n",
    "\n",
    "pad_token_id = phone_dict[\"PAD\"]\n",
    "for index in metadata.index:\n",
    "    phoneme = metadata[\"arpas\"][index].copy()\n",
    "\n",
    "    phoneme = [re.sub(\"\\d\", \"\", phn) for phn in phoneme]\n",
    "    phoneme = [phone_dict[phn] for phn in phoneme]\n",
    "    padding = [pad_token_id, ]*(MAX_LENGTH-len(phoneme))\n",
    "\n",
    "    phoneme += padding\n",
    "    phone_ids.append(torch.tensor(phoneme))\n",
    "\n",
    "phone_ids = torch.stack(phone_ids, dim=0)\n",
    "phone_ids = phone_ids.numpy()\n",
    "np.save(f'{out_dir}/phone_ids.npy', phone_ids)\n",
    "phone_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract WavLM Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /data/codes/prep_ps_pykaldi/wavlm\n",
    "import torch\n",
    "from wavlm import WavLM, WavLMConfig\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = \"/data/codes/prep_ps_pykaldi/pretrained/wavlm-base+.pt\"\n",
    "checkpoint = torch.load(pretrained_path)\n",
    "\n",
    "config = WavLMConfig(checkpoint['cfg'])\n",
    "model = WavLM(config).eval().cuda()\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(alignment, features):\n",
    "    index = 0\n",
    "    phonemes = []\n",
    "    indices = -1 * torch.ones(alignment[-1][1] + alignment[-1][2])\n",
    "    for phoneme, start_frame, duration in alignment:\n",
    "        if phoneme == \"SIL\":\n",
    "            continue\n",
    "        end_frame = start_frame + duration\n",
    "        indices[start_frame:end_frame] = index\n",
    "        phonemes.append(phoneme)\n",
    "        index += 1\n",
    "\n",
    "    indices[indices==-1] = indices.max() + 1\n",
    "\n",
    "    indices = torch.nn.functional.one_hot(indices.long(), num_classes=int(indices.max().item())+1).cuda()\n",
    "    indices = indices / indices.sum(0, keepdim=True)\n",
    "    \n",
    "    if features.shape[0] != indices.shape[0]:\n",
    "        features = features[0:indices.shape[0]]\n",
    "    features = torch.matmul(indices.transpose(0, 1), features)\n",
    "\n",
    "    return features[:-1].cpu(), phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_dir = \"/data/codes/prep_ps_pykaldi/prep_data/wav\"\n",
    "\n",
    "wavlm_features = []\n",
    "for index in tqdm(metadata.index):\n",
    "    wav_id = metadata[\"id\"][index]\n",
    "    alignment = metadata[\"alignment\"][index]\n",
    "\n",
    "    alignment = json.loads(alignment)\n",
    "    wav, sr = librosa.load(f'{wav_dir}/{wav_id}.wav', sr=16000)\n",
    "\n",
    "    input_values = torch.from_numpy(wav).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        features = model.extract_features(input_values)[0]\n",
    "\n",
    "    index = torch.arange(features.shape[1]).unsqueeze(-1).cuda()\n",
    "    expanded_index = index.expand((-1, 2)).flatten()\n",
    "    features = features[0][expanded_index]\n",
    "\n",
    "    features, phonemes = extract_feature(alignment, features)\n",
    "\n",
    "    features = torch.concat([features, torch.zeros(MAX_LENGTH-len(phonemes), 768)], axis=0)\n",
    "    wavlm_features.append(features)\n",
    "\n",
    "wavlm_features = torch.stack(wavlm_features, dim=0)\n",
    "wavlm_features = wavlm_features.numpy()\n",
    "np.save(f'{out_dir}/wavlm_features.npy', wavlm_features)\n",
    "wavlm_features = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Hubert Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, HubertModel\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "model = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\").eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(alignment, features):\n",
    "    index = 0\n",
    "    phonemes = []\n",
    "    indices = -1 * torch.ones(alignment[-1][1] + alignment[-1][2])\n",
    "    for phoneme, start_frame, duration in alignment:\n",
    "        if phoneme == \"SIL\":\n",
    "            continue\n",
    "        end_frame = start_frame + duration\n",
    "        indices[start_frame:end_frame] = index\n",
    "        phonemes.append(phoneme)\n",
    "        index += 1\n",
    "\n",
    "    indices[indices==-1] = indices.max() + 1\n",
    "\n",
    "    indices = torch.nn.functional.one_hot(indices.long(), num_classes=int(indices.max().item())+1).cuda()\n",
    "    indices = indices / indices.sum(0, keepdim=True)\n",
    "    \n",
    "    if features.shape[0] != indices.shape[0]:\n",
    "        print(features.shape, indices.shape)\n",
    "        features = features[0:indices.shape[0]]\n",
    "    features = torch.matmul(indices.transpose(0, 1), features)\n",
    "\n",
    "    return features[:-1].cpu(), phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_dir = \"/data/codes/prep_ps_pykaldi/prep_data/wav\"\n",
    "\n",
    "hubert_features = []\n",
    "for index in tqdm(metadata.index):\n",
    "    wav_id = metadata[\"id\"][index]\n",
    "    alignment = metadata[\"alignment\"][index]\n",
    "\n",
    "    alignment = json.loads(alignment)\n",
    "    wav, sr = librosa.load(f'{wav_dir}/{wav_id}.wav', sr=16000)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = processor(wav, return_tensors=\"pt\", sampling_rate=16000)\n",
    "        features = model(features[\"input_values\"].cuda()).last_hidden_state\n",
    "\n",
    "    index = torch.arange(features.shape[1]).unsqueeze(-1).cuda()\n",
    "    expanded_index = index.expand((-1, 2)).flatten()\n",
    "    features = features[0][expanded_index]\n",
    "\n",
    "    features, phonemes = extract_feature(alignment, features)\n",
    "\n",
    "    features = torch.concat([features, torch.zeros(MAX_LENGTH-len(phonemes), 1024)], axis=0)\n",
    "    hubert_features.append(features)\n",
    "\n",
    "hubert_features = torch.stack(hubert_features, dim=0)\n",
    "hubert_features = hubert_features.numpy()\n",
    "# np.save(f'{out_dir}/hubert_features.npy', hubert_features)\n",
    "hubert_features = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Wav2vec Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2Model.from_pretrained(\"/data/codes/prep_ps_pykaldi/pretrained/charsiu\").eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(alignment, features):\n",
    "    index = 0\n",
    "    phonemes = []\n",
    "    indices = -1 * torch.ones(alignment[-1][1] + alignment[-1][2])\n",
    "    for phoneme, start_frame, duration in alignment:\n",
    "        if phoneme == \"SIL\":\n",
    "            continue\n",
    "        end_frame = start_frame + duration\n",
    "        indices[start_frame:end_frame] = index\n",
    "        phonemes.append(phoneme)\n",
    "        index += 1\n",
    "\n",
    "    indices[indices==-1] = indices.max() + 1\n",
    "\n",
    "    indices = torch.nn.functional.one_hot(indices.long(), num_classes=int(indices.max().item())+1).cuda()\n",
    "    indices = indices / indices.sum(0, keepdim=True)\n",
    "    \n",
    "    if features.shape[0] != indices.shape[0]:\n",
    "        print(features.shape[0], indices.shape[0])\n",
    "        features = features[0:indices.shape[0]]\n",
    "        print(\"Hello\")\n",
    "    features = torch.matmul(indices.transpose(0, 1), features)\n",
    "\n",
    "    return features[:-1].cpu(), phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_dir = \"/data/codes/prep_ps_pykaldi/prep_data/wav\"\n",
    "\n",
    "hubert_features = []\n",
    "for index in tqdm(metadata.index):\n",
    "    wav_id = metadata[\"id\"][index]\n",
    "    alignment = metadata[\"alignment\"][index]\n",
    "\n",
    "    alignment = json.loads(alignment)\n",
    "    wav, sr = librosa.load(f'{wav_dir}/{wav_id}.wav', sr=16000)\n",
    "    features = torch.from_numpy(wav).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        # features = processor(wav, return_tensors=\"pt\", sampling_rate=16000)\n",
    "        features = model(features).last_hidden_state\n",
    "        if index % 100 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    features, phonemes = extract_feature(alignment, features[0])\n",
    "\n",
    "    features = torch.concat([features, torch.zeros(MAX_LENGTH-len(phonemes), 768)], axis=0)\n",
    "    hubert_features.append(features)\n",
    "\n",
    "hubert_features = torch.stack(hubert_features, dim=0)\n",
    "hubert_features = hubert_features.numpy()\n",
    "# np.save(f'{out_dir}/wav2vec_features.npy', hubert_features)\n",
    "hubert_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
