{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/codes/prep_ps_pykaldi\n"
     ]
    }
   ],
   "source": [
    "%cd /data/codes/prep_ps_pykaldi/\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 64\n",
    "\n",
    "metadata_path = \"/data/codes/prep_ps_pykaldi/prep_data/jsonl_v1/info_in_domain_long_sentence_testset_old.jsonl\"\n",
    "align_path = \"/data/codes/prep_ps_pykaldi/exp/sm/prep/merged_align.out\"\n",
    "gop_path = '/data/codes/prep_ps_pykaldi/exp/sm/prep/merged_gop.pkl'\n",
    "out_dir = \"/data/codes/prep_ps_pykaldi/exp/sm/prep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>id</th>\n",
       "      <th>qid</th>\n",
       "      <th>text</th>\n",
       "      <th>arpas</th>\n",
       "      <th>phone_scores</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>trans</th>\n",
       "      <th>word_scores</th>\n",
       "      <th>utterance_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144801.0</td>\n",
       "      <td>4141169</td>\n",
       "      <td>67632</td>\n",
       "      <td>I THINK PORTRAITS ARE MORE BEAUTIFUL WHEN THEY...</td>\n",
       "      <td>[AY1, TH, IH1, NG, K, P, AO1, R, T, R, AH0, T,...</td>\n",
       "      <td>[100, 97, 96, 99, 82, 100, 10, 17, 95, 96, 0, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, ...</td>\n",
       "      <td>[AY, TH, IH, NG, K, P, AA, R, T, R, IY, T, S, ...</td>\n",
       "      <td>[92, 90, 35, 90, 92, 36, 92, 92, 66, 92, 92]</td>\n",
       "      <td>66.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98397.0</td>\n",
       "      <td>4145212</td>\n",
       "      <td>67393</td>\n",
       "      <td>SHE HAS A SOFT VOICE</td>\n",
       "      <td>[SH, IY1, HH, AE1, Z, AH0, S, AA1, F, T, V, OY...</td>\n",
       "      <td>[100, 100, 98, 96, 32, 100, 97, 48, 89, 100, 9...</td>\n",
       "      <td>[0, 0, 1, 1, 1, 2, 3, 3, 3, 3, 4, 4, 4]</td>\n",
       "      <td>[SH, IY, HH, AE, S, SCHWA, S, ER, F, T, V, OY, S]</td>\n",
       "      <td>[97, 64, 97, 75, 94]</td>\n",
       "      <td>84.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid       id    qid  \\\n",
       "0  144801.0  4141169  67632   \n",
       "1   98397.0  4145212  67393   \n",
       "\n",
       "                                                text  \\\n",
       "0  I THINK PORTRAITS ARE MORE BEAUTIFUL WHEN THEY...   \n",
       "1                               SHE HAS A SOFT VOICE   \n",
       "\n",
       "                                               arpas  \\\n",
       "0  [AY1, TH, IH1, NG, K, P, AO1, R, T, R, AH0, T,...   \n",
       "1  [SH, IY1, HH, AE1, Z, AH0, S, AA1, F, T, V, OY...   \n",
       "\n",
       "                                        phone_scores  \\\n",
       "0  [100, 97, 96, 99, 82, 100, 10, 17, 95, 96, 0, ...   \n",
       "1  [100, 100, 98, 96, 32, 100, 97, 48, 89, 100, 9...   \n",
       "\n",
       "                                            word_ids  \\\n",
       "0  [0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, ...   \n",
       "1            [0, 0, 1, 1, 1, 2, 3, 3, 3, 3, 4, 4, 4]   \n",
       "\n",
       "                                               trans  \\\n",
       "0  [AY, TH, IH, NG, K, P, AA, R, T, R, IY, T, S, ...   \n",
       "1  [SH, IY, HH, AE, S, SCHWA, S, ER, F, T, V, OY, S]   \n",
       "\n",
       "                                    word_scores  utterance_scores  \n",
       "0  [92, 90, 35, 90, 92, 36, 92, 92, 66, 92, 92]             66.63  \n",
       "1                          [97, 64, 97, 75, 94]             84.24  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [json.loads(line.strip()) for line in lines]\n",
    "    \n",
    "    lines = pd.DataFrame(lines)\n",
    "    return lines\n",
    "\n",
    "metadata = load_jsonl(metadata_path)\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>id</th>\n",
       "      <th>qid</th>\n",
       "      <th>text</th>\n",
       "      <th>arpas</th>\n",
       "      <th>phone_scores</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>trans</th>\n",
       "      <th>word_scores</th>\n",
       "      <th>utterance_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144801.0</td>\n",
       "      <td>4141169</td>\n",
       "      <td>67632</td>\n",
       "      <td>I THINK PORTRAITS ARE MORE BEAUTIFUL WHEN THEY...</td>\n",
       "      <td>[AY1, TH, IH1, NG, K, P, AO1, R, T, R, AH0, T,...</td>\n",
       "      <td>[100, 97, 96, 99, 82, 100, 10, 17, 95, 96, 0, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, ...</td>\n",
       "      <td>[AY, TH, IH, NG, K, P, AA, R, T, R, IY, T, S, ...</td>\n",
       "      <td>[92, 90, 35, 90, 92, 36, 92, 92, 66, 92, 92]</td>\n",
       "      <td>66.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98397.0</td>\n",
       "      <td>4145212</td>\n",
       "      <td>67393</td>\n",
       "      <td>SHE HAS A SOFT VOICE</td>\n",
       "      <td>[SH, IY1, HH, AE1, Z, AH0, S, AA1, F, T, V, OY...</td>\n",
       "      <td>[100, 100, 98, 96, 32, 100, 97, 48, 89, 100, 9...</td>\n",
       "      <td>[0, 0, 1, 1, 1, 2, 3, 3, 3, 3, 4, 4, 4]</td>\n",
       "      <td>[SH, IY, HH, AE, S, SCHWA, S, ER, F, T, V, OY, S]</td>\n",
       "      <td>[97, 64, 97, 75, 94]</td>\n",
       "      <td>84.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid       id    qid  \\\n",
       "0  144801.0  4141169  67632   \n",
       "1   98397.0  4145212  67393   \n",
       "\n",
       "                                                text  \\\n",
       "0  I THINK PORTRAITS ARE MORE BEAUTIFUL WHEN THEY...   \n",
       "1                               SHE HAS A SOFT VOICE   \n",
       "\n",
       "                                               arpas  \\\n",
       "0  [AY1, TH, IH1, NG, K, P, AO1, R, T, R, AH0, T,...   \n",
       "1  [SH, IY1, HH, AE1, Z, AH0, S, AA1, F, T, V, OY...   \n",
       "\n",
       "                                        phone_scores  \\\n",
       "0  [100, 97, 96, 99, 82, 100, 10, 17, 95, 96, 0, ...   \n",
       "1  [100, 100, 98, 96, 32, 100, 97, 48, 89, 100, 9...   \n",
       "\n",
       "                                            word_ids  \\\n",
       "0  [0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, ...   \n",
       "1            [0, 0, 1, 1, 1, 2, 3, 3, 3, 3, 4, 4, 4]   \n",
       "\n",
       "                                               trans  \\\n",
       "0  [AY, TH, IH, NG, K, P, AA, R, T, R, IY, T, S, ...   \n",
       "1  [SH, IY, HH, AE, S, SCHWA, S, ER, F, T, V, OY, S]   \n",
       "\n",
       "                                    word_scores  utterance_scores  \n",
       "0  [92, 90, 35, 90, 92, 36, 92, 92, 66, 92, 92]             66.63  \n",
       "1                          [97, 64, 97, 75, 94]             84.24  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(gop_path, 'rb') as f:\n",
    "    gop_features = pickle.load(f)\n",
    "    \n",
    "metadata = metadata[metadata.id.isin(gop_features)]\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>id</th>\n",
       "      <th>qid</th>\n",
       "      <th>text</th>\n",
       "      <th>arpas</th>\n",
       "      <th>phone_scores</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>trans</th>\n",
       "      <th>word_scores</th>\n",
       "      <th>utterance_scores</th>\n",
       "      <th>features</th>\n",
       "      <th>kaldi_phoneme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144801.0</td>\n",
       "      <td>4141169</td>\n",
       "      <td>67632</td>\n",
       "      <td>I THINK PORTRAITS ARE MORE BEAUTIFUL WHEN THEY...</td>\n",
       "      <td>[AY1, TH, IH1, NG, K, P, AO1, R, T, R, AH0, T,...</td>\n",
       "      <td>[100, 97, 96, 99, 82, 100, 10, 17, 95, 96, 0, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, ...</td>\n",
       "      <td>[AY, TH, IH, NG, K, P, AA, R, T, R, IY, T, S, ...</td>\n",
       "      <td>[92, 90, 35, 90, 92, 36, 92, 92, 66, 92, 92]</td>\n",
       "      <td>66.63</td>\n",
       "      <td>[[-7.036007999373453, -6.696772841758492, -3.3...</td>\n",
       "      <td>[AY, TH, IH, NG, K, P, AO, R, T, R, AH, T, S, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98397.0</td>\n",
       "      <td>4145212</td>\n",
       "      <td>67393</td>\n",
       "      <td>SHE HAS A SOFT VOICE</td>\n",
       "      <td>[SH, IY1, HH, AE1, Z, AH0, S, AA1, F, T, V, OY...</td>\n",
       "      <td>[100, 100, 98, 96, 32, 100, 97, 48, 89, 100, 9...</td>\n",
       "      <td>[0, 0, 1, 1, 1, 2, 3, 3, 3, 3, 4, 4, 4]</td>\n",
       "      <td>[SH, IY, HH, AE, S, SCHWA, S, ER, F, T, V, OY, S]</td>\n",
       "      <td>[97, 64, 97, 75, 94]</td>\n",
       "      <td>84.24</td>\n",
       "      <td>[[-7.743860181416019, -5.810622266743849, -3.9...</td>\n",
       "      <td>[SH, IY, HH, AE, Z, AH, S, AO, F, T, V, OY, S]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid       id    qid  \\\n",
       "0  144801.0  4141169  67632   \n",
       "1   98397.0  4145212  67393   \n",
       "\n",
       "                                                text  \\\n",
       "0  I THINK PORTRAITS ARE MORE BEAUTIFUL WHEN THEY...   \n",
       "1                               SHE HAS A SOFT VOICE   \n",
       "\n",
       "                                               arpas  \\\n",
       "0  [AY1, TH, IH1, NG, K, P, AO1, R, T, R, AH0, T,...   \n",
       "1  [SH, IY1, HH, AE1, Z, AH0, S, AA1, F, T, V, OY...   \n",
       "\n",
       "                                        phone_scores  \\\n",
       "0  [100, 97, 96, 99, 82, 100, 10, 17, 95, 96, 0, ...   \n",
       "1  [100, 100, 98, 96, 32, 100, 97, 48, 89, 100, 9...   \n",
       "\n",
       "                                            word_ids  \\\n",
       "0  [0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, ...   \n",
       "1            [0, 0, 1, 1, 1, 2, 3, 3, 3, 3, 4, 4, 4]   \n",
       "\n",
       "                                               trans  \\\n",
       "0  [AY, TH, IH, NG, K, P, AA, R, T, R, IY, T, S, ...   \n",
       "1  [SH, IY, HH, AE, S, SCHWA, S, ER, F, T, V, OY, S]   \n",
       "\n",
       "                                    word_scores  utterance_scores  \\\n",
       "0  [92, 90, 35, 90, 92, 36, 92, 92, 66, 92, 92]             66.63   \n",
       "1                          [97, 64, 97, 75, 94]             84.24   \n",
       "\n",
       "                                            features  \\\n",
       "0  [[-7.036007999373453, -6.696772841758492, -3.3...   \n",
       "1  [[-7.743860181416019, -5.810622266743849, -3.9...   \n",
       "\n",
       "                                       kaldi_phoneme  \n",
       "0  [AY, TH, IH, NG, K, P, AO, R, T, R, AH, T, S, ...  \n",
       "1     [SH, IY, HH, AE, Z, AH, S, AO, F, T, V, OY, S]  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_gop_feature(id):\n",
    "    sample = gop_features[str(id)]\n",
    "    features = [\n",
    "        np.array(feature) for feature, phoneme in zip(sample[\"gopt\"], sample[\"phones\"][0])\n",
    "        if phoneme != \"SIL\"\n",
    "    ]\n",
    "    return np.stack(features)\n",
    "\n",
    "def extract_phonemes(id):\n",
    "    sample = gop_features[str(id)]\n",
    "    phonemes = [\n",
    "        re.sub(\"\\d\", \"\",phoneme.split(\"_\")[0]) for phoneme in sample[\"phones\"][0]\n",
    "        if phoneme != \"SIL\"\n",
    "    ]\n",
    "    return phonemes\n",
    "\n",
    "metadata[\"features\"] = metadata.id.apply(lambda x: extract_gop_feature(x))\n",
    "metadata[\"kaldi_phoneme\"] = metadata.id.apply(lambda x: extract_phonemes(x))\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>alignment</th>\n",
       "      <th>durations</th>\n",
       "      <th>phonemes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4243017</td>\n",
       "      <td>[[\"SIL\", 0, 78], [\"W_B\", 78, 12], [\"IY1_E\", 90...</td>\n",
       "      <td>[0.24, 0.18, 0.18, 0.32, 0.2, 0.22, 0.2, 0.18,...</td>\n",
       "      <td>[W, IY1, HH, AE1, D, AH0, HH, AE1, P, IY0, HH,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1641228</td>\n",
       "      <td>[[\"SIL\", 0, 36], [\"L_B\", 36, 13], [\"EH1_I\", 49...</td>\n",
       "      <td>[0.26, 0.24, 0.16, 0.16, 0.32, 0.28, 0.46, 0.1...</td>\n",
       "      <td>[L, EH1, T, M, AY1, HH, EH1, R, D, AW1, N]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                          alignment  \\\n",
       "0  4243017  [[\"SIL\", 0, 78], [\"W_B\", 78, 12], [\"IY1_E\", 90...   \n",
       "1  1641228  [[\"SIL\", 0, 36], [\"L_B\", 36, 13], [\"EH1_I\", 49...   \n",
       "\n",
       "                                           durations  \\\n",
       "0  [0.24, 0.18, 0.18, 0.32, 0.2, 0.22, 0.2, 0.18,...   \n",
       "1  [0.26, 0.24, 0.16, 0.16, 0.32, 0.28, 0.46, 0.1...   \n",
       "\n",
       "                                            phonemes  \n",
       "0  [W, IY1, HH, AE1, D, AH0, HH, AE1, P, IY0, HH,...  \n",
       "1         [L, EH1, T, M, AY1, HH, EH1, R, D, AW1, N]  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_df = pd.read_csv(align_path, names=[\"id\", \"alignment\"], sep=\"\\t\")\n",
    "\n",
    "def extract_duration(alignment):\n",
    "    alignment = json.loads(alignment)\n",
    "    durations = []\n",
    "    \n",
    "    for phoneme, start, duration in alignment:\n",
    "        if phoneme == \"SIL\":\n",
    "            continue\n",
    "        durations.append(round(duration * 0.02, 4))\n",
    "\n",
    "    return durations\n",
    "\n",
    "def extract_phonemes(alignment):\n",
    "    alignment = json.loads(alignment)\n",
    "    phonemes = []\n",
    "    \n",
    "    for phoneme, start, duration in alignment:\n",
    "        if phoneme == \"SIL\":\n",
    "            continue\n",
    "        phonemes.append(phoneme.split(\"_\")[0])\n",
    "\n",
    "    return phonemes\n",
    "\n",
    "align_df[\"durations\"] = align_df[\"alignment\"].apply(lambda x: extract_duration(x))\n",
    "align_df[\"phonemes\"] = align_df[\"alignment\"].apply(lambda x: extract_phonemes(x))\n",
    "align_df[\"id\"] = align_df[\"id\"].apply(str)\n",
    "align_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>id</th>\n",
       "      <th>qid</th>\n",
       "      <th>text</th>\n",
       "      <th>arpas</th>\n",
       "      <th>phone_scores</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>trans</th>\n",
       "      <th>word_scores</th>\n",
       "      <th>utterance_scores</th>\n",
       "      <th>features</th>\n",
       "      <th>kaldi_phoneme</th>\n",
       "      <th>durations</th>\n",
       "      <th>alignment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144801.0</td>\n",
       "      <td>4141169</td>\n",
       "      <td>67632</td>\n",
       "      <td>I THINK PORTRAITS ARE MORE BEAUTIFUL WHEN THEY...</td>\n",
       "      <td>[AY1, TH, IH1, NG, K, P, AO1, R, T, R, AH0, T,...</td>\n",
       "      <td>[100, 97, 96, 99, 82, 100, 10, 17, 95, 96, 0, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, ...</td>\n",
       "      <td>[AY, TH, IH, NG, K, P, AA, R, T, R, IY, T, S, ...</td>\n",
       "      <td>[92, 90, 35, 90, 92, 36, 92, 92, 66, 92, 92]</td>\n",
       "      <td>66.63</td>\n",
       "      <td>[[-7.036007999373453, -6.696772841758492, -3.3...</td>\n",
       "      <td>[AY, TH, IH, NG, K, P, AO, R, T, R, AH, T, S, ...</td>\n",
       "      <td>[0.58, 0.46, 0.26, 0.34, 0.24, 0.3, 0.24, 0.1,...</td>\n",
       "      <td>[[\"SIL\", 0, 53], [\"AY1_S\", 53, 29], [\"TH_B\", 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98397.0</td>\n",
       "      <td>4145212</td>\n",
       "      <td>67393</td>\n",
       "      <td>SHE HAS A SOFT VOICE</td>\n",
       "      <td>[SH, IY1, HH, AE1, Z, AH0, S, AA1, F, T, V, OY...</td>\n",
       "      <td>[100, 100, 98, 96, 32, 100, 97, 48, 89, 100, 9...</td>\n",
       "      <td>[0, 0, 1, 1, 1, 2, 3, 3, 3, 3, 4, 4, 4]</td>\n",
       "      <td>[SH, IY, HH, AE, S, SCHWA, S, ER, F, T, V, OY, S]</td>\n",
       "      <td>[97, 64, 97, 75, 94]</td>\n",
       "      <td>84.24</td>\n",
       "      <td>[[-7.743860181416019, -5.810622266743849, -3.9...</td>\n",
       "      <td>[SH, IY, HH, AE, Z, AH, S, AO, F, T, V, OY, S]</td>\n",
       "      <td>[0.56, 0.24, 0.18, 0.42, 0.76, 0.16, 0.4, 0.34...</td>\n",
       "      <td>[[\"SIL\", 0, 30], [\"SH_B\", 30, 28], [\"IY1_E\", 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid       id    qid  \\\n",
       "0  144801.0  4141169  67632   \n",
       "1   98397.0  4145212  67393   \n",
       "\n",
       "                                                text  \\\n",
       "0  I THINK PORTRAITS ARE MORE BEAUTIFUL WHEN THEY...   \n",
       "1                               SHE HAS A SOFT VOICE   \n",
       "\n",
       "                                               arpas  \\\n",
       "0  [AY1, TH, IH1, NG, K, P, AO1, R, T, R, AH0, T,...   \n",
       "1  [SH, IY1, HH, AE1, Z, AH0, S, AA1, F, T, V, OY...   \n",
       "\n",
       "                                        phone_scores  \\\n",
       "0  [100, 97, 96, 99, 82, 100, 10, 17, 95, 96, 0, ...   \n",
       "1  [100, 100, 98, 96, 32, 100, 97, 48, 89, 100, 9...   \n",
       "\n",
       "                                            word_ids  \\\n",
       "0  [0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, ...   \n",
       "1            [0, 0, 1, 1, 1, 2, 3, 3, 3, 3, 4, 4, 4]   \n",
       "\n",
       "                                               trans  \\\n",
       "0  [AY, TH, IH, NG, K, P, AA, R, T, R, IY, T, S, ...   \n",
       "1  [SH, IY, HH, AE, S, SCHWA, S, ER, F, T, V, OY, S]   \n",
       "\n",
       "                                    word_scores  utterance_scores  \\\n",
       "0  [92, 90, 35, 90, 92, 36, 92, 92, 66, 92, 92]             66.63   \n",
       "1                          [97, 64, 97, 75, 94]             84.24   \n",
       "\n",
       "                                            features  \\\n",
       "0  [[-7.036007999373453, -6.696772841758492, -3.3...   \n",
       "1  [[-7.743860181416019, -5.810622266743849, -3.9...   \n",
       "\n",
       "                                       kaldi_phoneme  \\\n",
       "0  [AY, TH, IH, NG, K, P, AO, R, T, R, AH, T, S, ...   \n",
       "1     [SH, IY, HH, AE, Z, AH, S, AO, F, T, V, OY, S]   \n",
       "\n",
       "                                           durations  \\\n",
       "0  [0.58, 0.46, 0.26, 0.34, 0.24, 0.3, 0.24, 0.1,...   \n",
       "1  [0.56, 0.24, 0.18, 0.42, 0.76, 0.16, 0.4, 0.34...   \n",
       "\n",
       "                                           alignment  \n",
       "0  [[\"SIL\", 0, 53], [\"AY1_S\", 53, 29], [\"TH_B\", 8...  \n",
       "1  [[\"SIL\", 0, 30], [\"SH_B\", 30, 28], [\"IY1_E\", 5...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.merge(metadata, align_df[[\"id\", \"durations\", \"alignment\"]], how=\"left\", on=\"id\")\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metadata[\"length\"] = metadata[\"arpas\"].apply(len)\n",
    "metadata[\"length\"].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sentence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1560,)\n"
     ]
    }
   ],
   "source": [
    "sentence_scores = []\n",
    "\n",
    "for index in metadata.index:\n",
    "    sentence_score = metadata[\"utterance_scores\"][index].copy()\n",
    "\n",
    "    sentence_scores.append(sentence_score)\n",
    "\n",
    "sentence_scores = torch.tensor(sentence_scores)\n",
    "sentence_scores = sentence_scores.numpy()\n",
    "print(sentence_scores.shape)\n",
    "np.save(f'{out_dir}/sentence_scores.npy', sentence_scores)\n",
    "sentence_scores = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract word scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1560, 64)\n"
     ]
    }
   ],
   "source": [
    "word_scores = []\n",
    "\n",
    "for index in metadata.index:\n",
    "    word_score = metadata[\"word_scores\"][index].copy()\n",
    "    word_id = metadata[\"word_ids\"][index].copy()\n",
    "\n",
    "    word_score_in_phone_levels = []\n",
    "    for wid in word_id:\n",
    "        word_score_in_phone_levels.append(word_score[wid])\n",
    "\n",
    "    padding = [-1,]*(MAX_LENGTH-len(word_score_in_phone_levels))\n",
    "    word_score_in_phone_levels = word_score_in_phone_levels + padding\n",
    "    word_score_in_phone_levels = torch.tensor(word_score_in_phone_levels)\n",
    "    word_scores.append(word_score_in_phone_levels)\n",
    "\n",
    "word_scores = torch.stack(word_scores, dim=0)\n",
    "word_scores = word_scores.numpy()\n",
    "print(word_scores.shape)\n",
    "np.save(f'{out_dir}/word_scores.npy', word_scores)\n",
    "word_scores = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract word ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1560, 64)\n"
     ]
    }
   ],
   "source": [
    "word_ids = []\n",
    "\n",
    "for index in metadata.index:\n",
    "    word_id = metadata[\"word_ids\"][index].copy()\n",
    "\n",
    "    padding = [-1,]*(MAX_LENGTH-len(word_id))\n",
    "    word_id = word_id + padding\n",
    "    word_id = torch.tensor(word_id)\n",
    "    word_ids.append(word_id)\n",
    "\n",
    "word_ids = torch.stack(word_ids, dim=0)\n",
    "word_ids = word_ids.numpy()\n",
    "print(word_ids.shape)\n",
    "np.save(f'{out_dir}/word_ids.npy', word_ids)\n",
    "word_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract gop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1560, 64, 82)\n"
     ]
    }
   ],
   "source": [
    "gops = []\n",
    "\n",
    "for index in metadata.index:\n",
    "    gop = metadata[\"features\"][index].copy()\n",
    "\n",
    "    padding = [[0,]*len(gop[0]),]*(MAX_LENGTH-len(gop))\n",
    "    gop = gop.tolist() + padding\n",
    "    gop = torch.tensor(gop)\n",
    "    gops.append(gop)\n",
    "\n",
    "gops = torch.stack(gops, dim=0)\n",
    "gops = gops.numpy()\n",
    "print(gops.shape)\n",
    "np.save(f'{out_dir}/gop.npy', gops)\n",
    "gops = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = []\n",
    "\n",
    "for index in metadata.index:\n",
    "    duration = metadata[\"durations\"][index].copy()\n",
    "\n",
    "    padding = [0, ]*(MAX_LENGTH-len(duration))\n",
    "\n",
    "    duration += padding\n",
    "    duration = torch.tensor(duration)\n",
    "    durations.append(duration)\n",
    "\n",
    "durations = torch.stack(durations, dim=0)\n",
    "durations = durations.numpy()\n",
    "np.save(f'{out_dir}/duration.npy', durations)\n",
    "durations = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract phone scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_scores = []\n",
    "\n",
    "for index in metadata.index:\n",
    "    phone_score = metadata[\"phone_scores\"][index].copy()\n",
    "\n",
    "    padding = [-1, ]*(MAX_LENGTH-len(phone_score))\n",
    "\n",
    "    phone_score += padding\n",
    "    phone_score = torch.tensor(phone_score)\n",
    "    phone_scores.append(phone_score)\n",
    "\n",
    "phone_scores = torch.stack(phone_scores, dim=0)\n",
    "phone_scores = phone_scores.numpy()\n",
    "np.save(f'{out_dir}/phone_scores.npy', phone_scores)\n",
    "phone_scores = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract phone ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_dict_path =  \"/data/codes/prep_ps_pykaldi/resources/phone_dict.json\"\n",
    "with open(phone_dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    phone_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_ids = []\n",
    "\n",
    "pad_token_id = phone_dict[\"PAD\"]\n",
    "for index in metadata.index:\n",
    "    phoneme = metadata[\"arpas\"][index].copy()\n",
    "\n",
    "    phoneme = [re.sub(\"\\d\", \"\", phn) for phn in phoneme]\n",
    "    phoneme = [phone_dict[phn] for phn in phoneme]\n",
    "    padding = [pad_token_id, ]*(MAX_LENGTH-len(phoneme))\n",
    "\n",
    "    phoneme += padding\n",
    "    phone_ids.append(torch.tensor(phoneme))\n",
    "\n",
    "phone_ids = torch.stack(phone_ids, dim=0)\n",
    "phone_ids = phone_ids.numpy()\n",
    "np.save(f'{out_dir}/phone_ids.npy', phone_ids)\n",
    "phone_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract WavLM Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/codes/prep_ps_pykaldi/wavlm\n"
     ]
    }
   ],
   "source": [
    "%cd /data/codes/prep_ps_pykaldi/wavlm\n",
    "import torch\n",
    "from wavlm import WavLM, WavLMConfig\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_path = \"/data/codes/prep_ps_pykaldi/pretrained/wavlm-base+.pt\"\n",
    "checkpoint = torch.load(pretrained_path)\n",
    "\n",
    "config = WavLMConfig(checkpoint['cfg'])\n",
    "model = WavLM(config).eval().cuda()\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(alignment, features):\n",
    "    index = 0\n",
    "    phonemes = []\n",
    "    indices = -1 * torch.ones(alignment[-1][1] + alignment[-1][2])\n",
    "    for phoneme, start_frame, duration in alignment:\n",
    "        if phoneme == \"SIL\":\n",
    "            continue\n",
    "        end_frame = start_frame + duration\n",
    "        indices[start_frame:end_frame] = index\n",
    "        phonemes.append(phoneme)\n",
    "        index += 1\n",
    "\n",
    "    indices[indices==-1] = indices.max() + 1\n",
    "\n",
    "    indices = torch.nn.functional.one_hot(indices.long(), num_classes=int(indices.max().item())+1).cuda()\n",
    "    indices = indices / indices.sum(0, keepdim=True)\n",
    "    \n",
    "    if features.shape[0] != indices.shape[0]:\n",
    "        features = features[0:indices.shape[0]]\n",
    "    features = torch.matmul(indices.transpose(0, 1), features)\n",
    "\n",
    "    return features[:-1].cpu(), phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1560 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1560/1560 [00:11<00:00, 134.18it/s]\n"
     ]
    }
   ],
   "source": [
    "wav_dir = \"/data/codes/prep_ps_pykaldi/prep_data/wav\"\n",
    "\n",
    "wavlm_features = []\n",
    "for index in tqdm(metadata.index):\n",
    "    wav_id = metadata[\"id\"][index]\n",
    "    alignment = metadata[\"alignment\"][index]\n",
    "\n",
    "    alignment = json.loads(alignment)\n",
    "    wav, sr = librosa.load(f'{wav_dir}/{wav_id}.wav', sr=16000)\n",
    "\n",
    "    input_values = torch.from_numpy(wav).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        features = model.extract_features(input_values)[0]\n",
    "\n",
    "    index = torch.arange(features.shape[1]).unsqueeze(-1).cuda()\n",
    "    expanded_index = index.expand((-1, 2)).flatten()\n",
    "    features = features[0][expanded_index]\n",
    "\n",
    "    features, phonemes = extract_feature(alignment, features)\n",
    "\n",
    "    features = torch.concat([features, torch.zeros(MAX_LENGTH-len(phonemes), 768)], axis=0)\n",
    "    wavlm_features.append(features)\n",
    "\n",
    "wavlm_features = torch.stack(wavlm_features, dim=0)\n",
    "wavlm_features = wavlm_features.numpy()\n",
    "np.save(f'{out_dir}/wavlm_features.npy', wavlm_features)\n",
    "wavlm_features = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Hubert Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, HubertModel\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "model = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\").eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(alignment, features):\n",
    "    index = 0\n",
    "    phonemes = []\n",
    "    indices = -1 * torch.ones(alignment[-1][1] + alignment[-1][2])\n",
    "    for phoneme, start_frame, duration in alignment:\n",
    "        if phoneme == \"SIL\":\n",
    "            continue\n",
    "        end_frame = start_frame + duration\n",
    "        indices[start_frame:end_frame] = index\n",
    "        phonemes.append(phoneme)\n",
    "        index += 1\n",
    "\n",
    "    indices[indices==-1] = indices.max() + 1\n",
    "\n",
    "    indices = torch.nn.functional.one_hot(indices.long(), num_classes=int(indices.max().item())+1).cuda()\n",
    "    indices = indices / indices.sum(0, keepdim=True)\n",
    "    \n",
    "    if features.shape[0] != indices.shape[0]:\n",
    "        print(features.shape, indices.shape)\n",
    "        features = features[0:indices.shape[0]]\n",
    "    features = torch.matmul(indices.transpose(0, 1), features)\n",
    "\n",
    "    return features[:-1].cpu(), phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 22/1560 [00:00<00:14, 109.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([852, 1024]) torch.Size([851, 43])\n",
      "torch.Size([398, 1024]) torch.Size([397, 14])\n",
      "torch.Size([246, 1024]) torch.Size([245, 12])\n",
      "torch.Size([246, 1024]) torch.Size([245, 12])\n",
      "torch.Size([246, 1024]) torch.Size([245, 12])\n",
      "torch.Size([204, 1024]) torch.Size([203, 15])\n",
      "torch.Size([170, 1024]) torch.Size([169, 13])\n",
      "torch.Size([298, 1024]) torch.Size([297, 21])\n",
      "torch.Size([332, 1024]) torch.Size([331, 15])\n",
      "torch.Size([434, 1024]) torch.Size([433, 14])\n",
      "torch.Size([400, 1024]) torch.Size([399, 19])\n",
      "torch.Size([668, 1024]) torch.Size([667, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 33/1560 [00:00<00:16, 93.47it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1296, 1024]) torch.Size([1295, 18])\n",
      "torch.Size([494, 1024]) torch.Size([493, 17])\n",
      "torch.Size([306, 1024]) torch.Size([305, 17])\n",
      "torch.Size([272, 1024]) torch.Size([271, 17])\n",
      "torch.Size([630, 1024]) torch.Size([629, 20])\n",
      "torch.Size([562, 1024]) torch.Size([561, 43])\n",
      "torch.Size([264, 1024]) torch.Size([263, 15])\n",
      "torch.Size([238, 1024]) torch.Size([237, 16])\n",
      "torch.Size([272, 1024]) torch.Size([271, 16])\n",
      "torch.Size([400, 1024]) torch.Size([399, 17])\n",
      "torch.Size([460, 1024]) torch.Size([459, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 54/1560 [00:00<00:15, 95.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 1024]) torch.Size([399, 14])\n",
      "torch.Size([392, 1024]) torch.Size([391, 15])\n",
      "torch.Size([272, 1024]) torch.Size([271, 20])\n",
      "torch.Size([434, 1024]) torch.Size([433, 29])\n",
      "torch.Size([328, 1024]) torch.Size([327, 17])\n",
      "torch.Size([300, 1024]) torch.Size([299, 18])\n",
      "torch.Size([366, 1024]) torch.Size([365, 12])\n",
      "torch.Size([426, 1024]) torch.Size([425, 13])\n",
      "torch.Size([622, 1024]) torch.Size([621, 12])\n",
      "torch.Size([340, 1024]) torch.Size([339, 12])\n",
      "torch.Size([170, 1024]) torch.Size([169, 16])\n",
      "torch.Size([306, 1024]) torch.Size([305, 12])\n",
      "torch.Size([332, 1024]) torch.Size([331, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 89/1560 [00:00<00:14, 104.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([332, 1024]) torch.Size([331, 16])\n",
      "torch.Size([264, 1024]) torch.Size([263, 18])\n",
      "torch.Size([298, 1024]) torch.Size([297, 18])\n",
      "torch.Size([238, 1024]) torch.Size([237, 18])\n",
      "torch.Size([238, 1024]) torch.Size([237, 18])\n",
      "torch.Size([306, 1024]) torch.Size([305, 17])\n",
      "torch.Size([366, 1024]) torch.Size([365, 23])\n",
      "torch.Size([298, 1024]) torch.Size([297, 18])\n",
      "torch.Size([298, 1024]) torch.Size([297, 17])\n",
      "torch.Size([332, 1024]) torch.Size([331, 23])\n",
      "torch.Size([426, 1024]) torch.Size([425, 16])\n",
      "torch.Size([878, 1024]) torch.Size([877, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 113/1560 [00:01<00:13, 105.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([306, 1024]) torch.Size([305, 12])\n",
      "torch.Size([246, 1024]) torch.Size([245, 12])\n",
      "torch.Size([246, 1024]) torch.Size([245, 12])\n",
      "torch.Size([584, 1024]) torch.Size([583, 22])\n",
      "torch.Size([212, 1024]) torch.Size([211, 12])\n",
      "torch.Size([332, 1024]) torch.Size([331, 13])\n",
      "torch.Size([272, 1024]) torch.Size([271, 12])\n",
      "torch.Size([440, 1024]) torch.Size([439, 16])\n",
      "torch.Size([366, 1024]) torch.Size([365, 16])\n",
      "torch.Size([332, 1024]) torch.Size([331, 16])\n",
      "torch.Size([332, 1024]) torch.Size([331, 12])\n",
      "torch.Size([264, 1024]) torch.Size([263, 12])\n",
      "torch.Size([298, 1024]) torch.Size([297, 12])\n",
      "torch.Size([980, 1024]) torch.Size([979, 43])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 124/1560 [00:01<00:14, 101.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([946, 1024]) torch.Size([945, 43])\n",
      "torch.Size([268, 1024]) torch.Size([267, 15])\n",
      "torch.Size([398, 1024]) torch.Size([397, 16])\n",
      "torch.Size([238, 1024]) torch.Size([237, 14])\n",
      "torch.Size([144, 1024]) torch.Size([143, 13])\n",
      "torch.Size([410, 1024]) torch.Size([409, 20])\n",
      "torch.Size([554, 1024]) torch.Size([553, 24])\n",
      "torch.Size([366, 1024]) torch.Size([365, 16])\n",
      "torch.Size([468, 1024]) torch.Size([467, 16])\n",
      "torch.Size([426, 1024]) torch.Size([425, 16])\n",
      "torch.Size([400, 1024]) torch.Size([399, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 146/1560 [00:01<00:14, 99.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([380, 1024]) torch.Size([379, 16])\n",
      "torch.Size([306, 1024]) torch.Size([305, 16])\n",
      "torch.Size([278, 1024]) torch.Size([277, 21])\n",
      "torch.Size([724, 1024]) torch.Size([723, 41])\n",
      "torch.Size([340, 1024]) torch.Size([339, 13])\n",
      "torch.Size([400, 1024]) torch.Size([399, 16])\n",
      "torch.Size([366, 1024]) torch.Size([365, 13])\n",
      "torch.Size([306, 1024]) torch.Size([305, 20])\n",
      "torch.Size([520, 1024]) torch.Size([519, 16])\n",
      "torch.Size([716, 1024]) torch.Size([715, 16])\n",
      "torch.Size([588, 1024]) torch.Size([587, 16])\n",
      "torch.Size([494, 1024]) torch.Size([493, 20])\n",
      "torch.Size([212, 1024]) torch.Size([211, 13])\n",
      "torch.Size([426, 1024]) torch.Size([425, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 169/1560 [00:01<00:13, 100.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([340, 1024]) torch.Size([339, 12])\n",
      "torch.Size([392, 1024]) torch.Size([391, 12])\n",
      "torch.Size([332, 1024]) torch.Size([331, 15])\n",
      "torch.Size([400, 1024]) torch.Size([399, 16])\n",
      "torch.Size([240, 1024]) torch.Size([239, 20])\n",
      "torch.Size([272, 1024]) torch.Size([271, 20])\n",
      "torch.Size([298, 1024]) torch.Size([297, 20])\n",
      "torch.Size([298, 1024]) torch.Size([297, 20])\n",
      "torch.Size([306, 1024]) torch.Size([305, 16])\n",
      "torch.Size([426, 1024]) torch.Size([425, 17])\n",
      "torch.Size([520, 1024]) torch.Size([519, 29])\n",
      "torch.Size([562, 1024]) torch.Size([561, 29])\n",
      "torch.Size([392, 1024]) torch.Size([391, 17])\n",
      "torch.Size([366, 1024]) torch.Size([365, 21])\n",
      "torch.Size([426, 1024]) torch.Size([425, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 191/1560 [00:01<00:13, 102.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 1024]) torch.Size([399, 14])\n",
      "torch.Size([374, 1024]) torch.Size([373, 14])\n",
      "torch.Size([340, 1024]) torch.Size([339, 14])\n",
      "torch.Size([554, 1024]) torch.Size([553, 21])\n",
      "torch.Size([366, 1024]) torch.Size([365, 15])\n",
      "torch.Size([366, 1024]) torch.Size([365, 16])\n",
      "torch.Size([332, 1024]) torch.Size([331, 13])\n",
      "torch.Size([460, 1024]) torch.Size([459, 15])\n",
      "torch.Size([460, 1024]) torch.Size([459, 16])\n",
      "torch.Size([426, 1024]) torch.Size([425, 13])\n",
      "torch.Size([434, 1024]) torch.Size([433, 16])\n",
      "torch.Size([332, 1024]) torch.Size([331, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 214/1560 [00:02<00:12, 103.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([562, 1024]) torch.Size([561, 16])\n",
      "torch.Size([596, 1024]) torch.Size([595, 19])\n",
      "torch.Size([784, 1024]) torch.Size([783, 32])\n",
      "torch.Size([212, 1024]) torch.Size([211, 12])\n",
      "torch.Size([212, 1024]) torch.Size([211, 12])\n",
      "torch.Size([264, 1024]) torch.Size([263, 13])\n",
      "torch.Size([246, 1024]) torch.Size([245, 12])\n",
      "torch.Size([332, 1024]) torch.Size([331, 13])\n",
      "torch.Size([264, 1024]) torch.Size([263, 12])\n",
      "torch.Size([246, 1024]) torch.Size([245, 12])\n",
      "torch.Size([332, 1024]) torch.Size([331, 19])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 236/1560 [00:02<00:12, 101.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([434, 1024]) torch.Size([433, 15])\n",
      "torch.Size([750, 1024]) torch.Size([749, 15])\n",
      "torch.Size([554, 1024]) torch.Size([553, 21])\n",
      "torch.Size([392, 1024]) torch.Size([391, 16])\n",
      "torch.Size([374, 1024]) torch.Size([373, 17])\n",
      "torch.Size([306, 1024]) torch.Size([305, 14])\n",
      "torch.Size([272, 1024]) torch.Size([271, 17])\n",
      "torch.Size([400, 1024]) torch.Size([399, 16])\n",
      "torch.Size([392, 1024]) torch.Size([391, 15])\n",
      "torch.Size([502, 1024]) torch.Size([501, 16])\n",
      "torch.Size([272, 1024]) torch.Size([271, 18])\n",
      "torch.Size([332, 1024]) torch.Size([331, 16])\n",
      "torch.Size([528, 1024]) torch.Size([527, 14])\n",
      "torch.Size([408, 1024]) torch.Size([407, 16])\n",
      "torch.Size([398, 1024]) torch.Size([397, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 260/1560 [00:02<00:12, 107.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([338, 1024]) torch.Size([337, 13])\n",
      "torch.Size([272, 1024]) torch.Size([271, 13])\n",
      "torch.Size([388, 1024]) torch.Size([387, 13])\n",
      "torch.Size([250, 1024]) torch.Size([249, 12])\n",
      "torch.Size([258, 1024]) torch.Size([257, 13])\n",
      "torch.Size([268, 1024]) torch.Size([267, 20])\n",
      "torch.Size([306, 1024]) torch.Size([305, 17])\n",
      "torch.Size([298, 1024]) torch.Size([297, 13])\n",
      "torch.Size([246, 1024]) torch.Size([245, 13])\n",
      "torch.Size([292, 1024]) torch.Size([291, 17])\n",
      "torch.Size([238, 1024]) torch.Size([237, 16])\n",
      "torch.Size([304, 1024]) torch.Size([303, 17])\n",
      "torch.Size([528, 1024]) torch.Size([527, 27])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 283/1560 [00:02<00:11, 109.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([298, 1024]) torch.Size([297, 17])\n",
      "torch.Size([682, 1024]) torch.Size([681, 29])\n",
      "torch.Size([340, 1024]) torch.Size([339, 17])\n",
      "torch.Size([400, 1024]) torch.Size([399, 14])\n",
      "torch.Size([264, 1024]) torch.Size([263, 14])\n",
      "torch.Size([400, 1024]) torch.Size([399, 14])\n",
      "torch.Size([264, 1024]) torch.Size([263, 14])\n",
      "torch.Size([272, 1024]) torch.Size([271, 15])\n",
      "torch.Size([306, 1024]) torch.Size([305, 14])\n",
      "torch.Size([298, 1024]) torch.Size([297, 15])\n",
      "torch.Size([392, 1024]) torch.Size([391, 15])\n",
      "torch.Size([494, 1024]) torch.Size([493, 15])\n",
      "torch.Size([426, 1024]) torch.Size([425, 14])\n",
      "torch.Size([272, 1024]) torch.Size([271, 12])\n",
      "torch.Size([502, 1024]) torch.Size([501, 30])\n",
      "torch.Size([340, 1024]) torch.Size([339, 24])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 305/1560 [00:02<00:11, 107.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([374, 1024]) torch.Size([373, 22])\n",
      "torch.Size([398, 1024]) torch.Size([397, 22])\n",
      "torch.Size([366, 1024]) torch.Size([365, 18])\n",
      "torch.Size([460, 1024]) torch.Size([459, 16])\n",
      "torch.Size([502, 1024]) torch.Size([501, 16])\n",
      "torch.Size([366, 1024]) torch.Size([365, 13])\n",
      "torch.Size([340, 1024]) torch.Size([339, 16])\n",
      "torch.Size([434, 1024]) torch.Size([433, 16])\n",
      "torch.Size([400, 1024]) torch.Size([399, 19])\n",
      "torch.Size([264, 1024]) torch.Size([263, 12])\n",
      "torch.Size([246, 1024]) torch.Size([245, 12])\n",
      "torch.Size([222, 1024]) torch.Size([221, 13])\n",
      "torch.Size([222, 1024]) torch.Size([221, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 331/1560 [00:03<00:10, 115.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([178, 1024]) torch.Size([177, 12])\n",
      "torch.Size([306, 1024]) torch.Size([305, 13])\n",
      "torch.Size([246, 1024]) torch.Size([245, 12])\n",
      "torch.Size([204, 1024]) torch.Size([203, 12])\n",
      "torch.Size([212, 1024]) torch.Size([211, 13])\n",
      "torch.Size([238, 1024]) torch.Size([237, 12])\n",
      "torch.Size([246, 1024]) torch.Size([245, 13])\n",
      "torch.Size([250, 1024]) torch.Size([249, 17])\n",
      "torch.Size([374, 1024]) torch.Size([373, 14])\n",
      "torch.Size([556, 1024]) torch.Size([555, 43])\n",
      "torch.Size([298, 1024]) torch.Size([297, 20])\n",
      "torch.Size([238, 1024]) torch.Size([237, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 355/1560 [00:03<00:10, 109.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 1024]) torch.Size([399, 20])\n",
      "torch.Size([400, 1024]) torch.Size([399, 18])\n",
      "torch.Size([334, 1024]) torch.Size([333, 16])\n",
      "torch.Size([434, 1024]) torch.Size([433, 15])\n",
      "torch.Size([392, 1024]) torch.Size([391, 14])\n",
      "torch.Size([238, 1024]) torch.Size([237, 18])\n",
      "torch.Size([366, 1024]) torch.Size([365, 16])\n",
      "torch.Size([194, 1024]) torch.Size([193, 12])\n",
      "torch.Size([194, 1024]) torch.Size([193, 12])\n",
      "torch.Size([222, 1024]) torch.Size([221, 12])\n",
      "torch.Size([240, 1024]) torch.Size([239, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 379/1560 [00:03<00:10, 108.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([204, 1024]) torch.Size([203, 17])\n",
      "torch.Size([878, 1024]) torch.Size([877, 24])\n",
      "torch.Size([400, 1024]) torch.Size([399, 20])\n",
      "torch.Size([298, 1024]) torch.Size([297, 20])\n",
      "torch.Size([298, 1024]) torch.Size([297, 20])\n",
      "torch.Size([306, 1024]) torch.Size([305, 20])\n",
      "torch.Size([340, 1024]) torch.Size([339, 17])\n",
      "torch.Size([366, 1024]) torch.Size([365, 13])\n",
      "torch.Size([298, 1024]) torch.Size([297, 13])\n",
      "torch.Size([366, 1024]) torch.Size([365, 13])\n",
      "torch.Size([340, 1024]) torch.Size([339, 12])\n",
      "torch.Size([306, 1024]) torch.Size([305, 16])\n",
      "torch.Size([298, 1024]) torch.Size([297, 16])\n",
      "torch.Size([400, 1024]) torch.Size([399, 16])\n",
      "torch.Size([264, 1024]) torch.Size([263, 14])\n",
      "torch.Size([570, 1024]) torch.Size([569, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 402/1560 [00:03<00:10, 106.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([946, 1024]) torch.Size([945, 25])\n",
      "torch.Size([332, 1024]) torch.Size([331, 14])\n",
      "torch.Size([332, 1024]) torch.Size([331, 14])\n",
      "torch.Size([340, 1024]) torch.Size([339, 13])\n",
      "torch.Size([332, 1024]) torch.Size([331, 12])\n",
      "torch.Size([332, 1024]) torch.Size([331, 13])\n",
      "torch.Size([246, 1024]) torch.Size([245, 12])\n",
      "torch.Size([272, 1024]) torch.Size([271, 15])\n",
      "torch.Size([306, 1024]) torch.Size([305, 21])\n",
      "torch.Size([374, 1024]) torch.Size([373, 20])\n",
      "torch.Size([234, 1024]) torch.Size([233, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 427/1560 [00:04<00:10, 104.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([332, 1024]) torch.Size([331, 20])\n",
      "torch.Size([520, 1024]) torch.Size([519, 18])\n",
      "torch.Size([502, 1024]) torch.Size([501, 17])\n",
      "torch.Size([494, 1024]) torch.Size([493, 29])\n",
      "torch.Size([374, 1024]) torch.Size([373, 15])\n",
      "torch.Size([460, 1024]) torch.Size([459, 16])\n",
      "torch.Size([392, 1024]) torch.Size([391, 15])\n",
      "torch.Size([434, 1024]) torch.Size([433, 19])\n",
      "torch.Size([238, 1024]) torch.Size([237, 18])\n",
      "torch.Size([396, 1024]) torch.Size([395, 15])\n",
      "torch.Size([338, 1024]) torch.Size([337, 15])\n",
      "torch.Size([232, 1024]) torch.Size([231, 14])\n",
      "torch.Size([210, 1024]) torch.Size([209, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 449/1560 [00:04<00:10, 104.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([434, 1024]) torch.Size([433, 14])\n",
      "torch.Size([502, 1024]) torch.Size([501, 24])\n",
      "torch.Size([420, 1024]) torch.Size([419, 16])\n",
      "torch.Size([508, 1024]) torch.Size([507, 19])\n",
      "torch.Size([332, 1024]) torch.Size([331, 16])\n",
      "torch.Size([222, 1024]) torch.Size([221, 15])\n",
      "torch.Size([306, 1024]) torch.Size([305, 12])\n",
      "torch.Size([250, 1024]) torch.Size([249, 15])\n",
      "torch.Size([240, 1024]) torch.Size([239, 15])\n",
      "torch.Size([1072, 1024]) torch.Size([1071, 32])\n",
      "torch.Size([528, 1024]) torch.Size([527, 28])\n",
      "torch.Size([298, 1024]) torch.Size([297, 28])\n",
      "torch.Size([340, 1024]) torch.Size([339, 28])\n",
      "torch.Size([374, 1024]) torch.Size([373, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 472/1560 [00:04<00:10, 106.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([810, 1024]) torch.Size([809, 43])\n",
      "torch.Size([298, 1024]) torch.Size([297, 12])\n",
      "torch.Size([264, 1024]) torch.Size([263, 15])\n",
      "torch.Size([246, 1024]) torch.Size([245, 16])\n",
      "torch.Size([340, 1024]) torch.Size([339, 13])\n",
      "torch.Size([298, 1024]) torch.Size([297, 13])\n",
      "torch.Size([332, 1024]) torch.Size([331, 12])\n",
      "torch.Size([278, 1024]) torch.Size([277, 13])\n",
      "torch.Size([246, 1024]) torch.Size([245, 17])\n",
      "torch.Size([562, 1024]) torch.Size([561, 43])\n",
      "torch.Size([366, 1024]) torch.Size([365, 28])\n",
      "torch.Size([426, 1024]) torch.Size([425, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 494/1560 [00:04<00:10, 106.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([306, 1024]) torch.Size([305, 20])\n",
      "torch.Size([366, 1024]) torch.Size([365, 13])\n",
      "torch.Size([460, 1024]) torch.Size([459, 13])\n",
      "torch.Size([352, 1024]) torch.Size([351, 12])\n",
      "torch.Size([332, 1024]) torch.Size([331, 16])\n",
      "torch.Size([272, 1024]) torch.Size([271, 16])\n",
      "torch.Size([554, 1024]) torch.Size([553, 30])\n",
      "torch.Size([340, 1024]) torch.Size([339, 16])\n",
      "torch.Size([238, 1024]) torch.Size([237, 17])\n",
      "torch.Size([554, 1024]) torch.Size([553, 30])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 516/1560 [00:04<00:09, 106.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([306, 1024]) torch.Size([305, 29])\n",
      "torch.Size([374, 1024]) torch.Size([373, 15])\n",
      "torch.Size([468, 1024]) torch.Size([467, 15])\n",
      "torch.Size([366, 1024]) torch.Size([365, 15])\n",
      "torch.Size([426, 1024]) torch.Size([425, 15])\n",
      "torch.Size([278, 1024]) torch.Size([277, 16])\n",
      "torch.Size([380, 1024]) torch.Size([379, 17])\n",
      "torch.Size([502, 1024]) torch.Size([501, 21])\n",
      "torch.Size([460, 1024]) torch.Size([459, 16])\n",
      "torch.Size([374, 1024]) torch.Size([373, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 539/1560 [00:05<00:09, 107.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([306, 1024]) torch.Size([305, 15])\n",
      "torch.Size([298, 1024]) torch.Size([297, 16])\n",
      "torch.Size([340, 1024]) torch.Size([339, 16])\n",
      "torch.Size([434, 1024]) torch.Size([433, 19])\n",
      "torch.Size([426, 1024]) torch.Size([425, 19])\n",
      "torch.Size([240, 1024]) torch.Size([239, 20])\n",
      "torch.Size([530, 1024]) torch.Size([529, 25])\n",
      "torch.Size([494, 1024]) torch.Size([493, 15])\n",
      "torch.Size([426, 1024]) torch.Size([425, 16])\n",
      "torch.Size([392, 1024]) torch.Size([391, 15])\n",
      "torch.Size([468, 1024]) torch.Size([467, 16])\n",
      "torch.Size([212, 1024]) torch.Size([211, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 561/1560 [00:05<00:09, 100.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([374, 1024]) torch.Size([373, 16])\n",
      "torch.Size([272, 1024]) torch.Size([271, 13])\n",
      "torch.Size([520, 1024]) torch.Size([519, 17])\n",
      "torch.Size([656, 1024]) torch.Size([655, 42])\n",
      "torch.Size([366, 1024]) torch.Size([365, 18])\n",
      "torch.Size([400, 1024]) torch.Size([399, 18])\n",
      "torch.Size([460, 1024]) torch.Size([459, 18])\n",
      "torch.Size([434, 1024]) torch.Size([433, 18])\n",
      "torch.Size([332, 1024]) torch.Size([331, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 582/1560 [00:05<00:10, 91.70it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([366, 1024]) torch.Size([365, 13])\n",
      "torch.Size([460, 1024]) torch.Size([459, 15])\n",
      "torch.Size([366, 1024]) torch.Size([365, 16])\n",
      "torch.Size([374, 1024]) torch.Size([373, 13])\n",
      "torch.Size([468, 1024]) torch.Size([467, 16])\n",
      "torch.Size([332, 1024]) torch.Size([331, 13])\n",
      "torch.Size([460, 1024]) torch.Size([459, 16])\n",
      "torch.Size([528, 1024]) torch.Size([527, 16])\n",
      "torch.Size([528, 1024]) torch.Size([527, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 592/1560 [00:05<00:13, 70.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([502, 1024]) torch.Size([501, 15])\n",
      "torch.Size([502, 1024]) torch.Size([501, 30])\n",
      "torch.Size([520, 1024]) torch.Size([519, 30])\n",
      "torch.Size([306, 1024]) torch.Size([305, 15])\n",
      "torch.Size([298, 1024]) torch.Size([297, 15])\n",
      "torch.Size([306, 1024]) torch.Size([305, 17])\n",
      "torch.Size([392, 1024]) torch.Size([391, 16])\n",
      "torch.Size([332, 1024]) torch.Size([331, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 610/1560 [00:06<00:12, 76.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([298, 1024]) torch.Size([297, 20])\n",
      "torch.Size([392, 1024]) torch.Size([391, 28])\n",
      "torch.Size([716, 1024]) torch.Size([715, 43])\n",
      "torch.Size([682, 1024]) torch.Size([681, 43])\n",
      "torch.Size([374, 1024]) torch.Size([373, 17])\n",
      "torch.Size([392, 1024]) torch.Size([391, 19])\n",
      "torch.Size([366, 1024]) torch.Size([365, 20])\n",
      "torch.Size([340, 1024]) torch.Size([339, 20])\n",
      "torch.Size([392, 1024]) torch.Size([391, 20])\n",
      "torch.Size([366, 1024]) torch.Size([365, 20])\n",
      "torch.Size([298, 1024]) torch.Size([297, 20])\n",
      "torch.Size([264, 1024]) torch.Size([263, 13])\n",
      "torch.Size([264, 1024]) torch.Size([263, 13])\n",
      "torch.Size([298, 1024]) torch.Size([297, 19])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 628/1560 [00:06<00:11, 82.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([398, 1024]) torch.Size([397, 16])\n",
      "torch.Size([502, 1024]) torch.Size([501, 20])\n",
      "torch.Size([714, 1024]) torch.Size([713, 43])\n",
      "torch.Size([298, 1024]) torch.Size([297, 17])\n",
      "torch.Size([400, 1024]) torch.Size([399, 29])\n",
      "torch.Size([426, 1024]) torch.Size([425, 29])\n",
      "torch.Size([306, 1024]) torch.Size([305, 20])\n",
      "torch.Size([332, 1024]) torch.Size([331, 16])\n",
      "torch.Size([434, 1024]) torch.Size([433, 16])\n",
      "torch.Size([298, 1024]) torch.Size([297, 20])\n",
      "torch.Size([426, 1024]) torch.Size([425, 21])\n",
      "torch.Size([426, 1024]) torch.Size([425, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 646/1560 [00:06<00:11, 82.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([426, 1024]) torch.Size([425, 21])\n",
      "torch.Size([222, 1024]) torch.Size([221, 12])\n",
      "torch.Size([222, 1024]) torch.Size([221, 13])\n",
      "torch.Size([250, 1024]) torch.Size([249, 13])\n",
      "torch.Size([332, 1024]) torch.Size([331, 20])\n",
      "torch.Size([400, 1024]) torch.Size([399, 13])\n",
      "torch.Size([306, 1024]) torch.Size([305, 12])\n",
      "torch.Size([400, 1024]) torch.Size([399, 13])\n",
      "torch.Size([366, 1024]) torch.Size([365, 12])\n",
      "torch.Size([434, 1024]) torch.Size([433, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 664/1560 [00:06<00:11, 81.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([392, 1024]) torch.Size([391, 12])\n",
      "torch.Size([298, 1024]) torch.Size([297, 12])\n",
      "torch.Size([392, 1024]) torch.Size([391, 16])\n",
      "torch.Size([400, 1024]) torch.Size([399, 18])\n",
      "torch.Size([340, 1024]) torch.Size([339, 12])\n",
      "torch.Size([340, 1024]) torch.Size([339, 13])\n",
      "torch.Size([264, 1024]) torch.Size([263, 12])\n",
      "torch.Size([366, 1024]) torch.Size([365, 13])\n",
      "torch.Size([178, 1024]) torch.Size([177, 12])\n",
      "torch.Size([528, 1024]) torch.Size([527, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 685/1560 [00:06<00:09, 91.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([426, 1024]) torch.Size([425, 18])\n",
      "torch.Size([374, 1024]) torch.Size([373, 18])\n",
      "torch.Size([426, 1024]) torch.Size([425, 18])\n",
      "torch.Size([250, 1024]) torch.Size([249, 12])\n",
      "torch.Size([346, 1024]) torch.Size([345, 15])\n",
      "torch.Size([262, 1024]) torch.Size([261, 12])\n",
      "torch.Size([460, 1024]) torch.Size([459, 15])\n",
      "torch.Size([326, 1024]) torch.Size([325, 15])\n",
      "torch.Size([358, 1024]) torch.Size([357, 12])\n",
      "torch.Size([422, 1024]) torch.Size([421, 15])\n",
      "torch.Size([296, 1024]) torch.Size([295, 12])\n",
      "torch.Size([416, 1024]) torch.Size([415, 13])\n",
      "torch.Size([438, 1024]) torch.Size([437, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 706/1560 [00:07<00:09, 93.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([366, 1024]) torch.Size([365, 25])\n",
      "torch.Size([306, 1024]) torch.Size([305, 15])\n",
      "torch.Size([332, 1024]) torch.Size([331, 16])\n",
      "torch.Size([298, 1024]) torch.Size([297, 15])\n",
      "torch.Size([246, 1024]) torch.Size([245, 16])\n",
      "torch.Size([400, 1024]) torch.Size([399, 19])\n",
      "torch.Size([306, 1024]) torch.Size([305, 15])\n",
      "torch.Size([426, 1024]) torch.Size([425, 13])\n",
      "torch.Size([400, 1024]) torch.Size([399, 19])\n",
      "torch.Size([400, 1024]) torch.Size([399, 15])\n",
      "torch.Size([204, 1024]) torch.Size([203, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 727/1560 [00:07<00:08, 95.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([306, 1024]) torch.Size([305, 13])\n",
      "torch.Size([374, 1024]) torch.Size([373, 13])\n",
      "torch.Size([298, 1024]) torch.Size([297, 19])\n",
      "torch.Size([332, 1024]) torch.Size([331, 13])\n",
      "torch.Size([392, 1024]) torch.Size([391, 19])\n",
      "torch.Size([340, 1024]) torch.Size([339, 13])\n",
      "torch.Size([468, 1024]) torch.Size([467, 21])\n",
      "torch.Size([366, 1024]) torch.Size([365, 20])\n",
      "torch.Size([340, 1024]) torch.Size([339, 20])\n",
      "torch.Size([332, 1024]) torch.Size([331, 17])\n",
      "torch.Size([460, 1024]) torch.Size([459, 30])\n",
      "torch.Size([460, 1024]) torch.Size([459, 29])\n",
      "torch.Size([332, 1024]) torch.Size([331, 24])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 750/1560 [00:07<00:07, 102.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([194, 1024]) torch.Size([193, 17])\n",
      "torch.Size([264, 1024]) torch.Size([263, 16])\n",
      "torch.Size([246, 1024]) torch.Size([245, 14])\n",
      "torch.Size([272, 1024]) torch.Size([271, 20])\n",
      "torch.Size([400, 1024]) torch.Size([399, 14])\n",
      "torch.Size([278, 1024]) torch.Size([277, 13])\n",
      "torch.Size([268, 1024]) torch.Size([267, 13])\n",
      "torch.Size([250, 1024]) torch.Size([249, 12])\n",
      "torch.Size([250, 1024]) torch.Size([249, 12])\n",
      "torch.Size([264, 1024]) torch.Size([263, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 774/1560 [00:07<00:07, 108.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([340, 1024]) torch.Size([339, 15])\n",
      "torch.Size([340, 1024]) torch.Size([339, 13])\n",
      "torch.Size([246, 1024]) torch.Size([245, 12])\n",
      "torch.Size([340, 1024]) torch.Size([339, 13])\n",
      "torch.Size([366, 1024]) torch.Size([365, 20])\n",
      "torch.Size([400, 1024]) torch.Size([399, 20])\n",
      "torch.Size([296, 1024]) torch.Size([295, 21])\n",
      "torch.Size([264, 1024]) torch.Size([263, 13])\n",
      "torch.Size([272, 1024]) torch.Size([271, 12])\n",
      "torch.Size([306, 1024]) torch.Size([305, 20])\n",
      "torch.Size([246, 1024]) torch.Size([245, 15])\n",
      "torch.Size([264, 1024]) torch.Size([263, 21])\n",
      "torch.Size([306, 1024]) torch.Size([305, 16])\n",
      "torch.Size([400, 1024]) torch.Size([399, 17])\n",
      "torch.Size([298, 1024]) torch.Size([297, 12])\n",
      "torch.Size([460, 1024]) torch.Size([459, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 797/1560 [00:08<00:07, 104.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([878, 1024]) torch.Size([877, 15])\n",
      "torch.Size([972, 1024]) torch.Size([971, 29])\n",
      "torch.Size([264, 1024]) torch.Size([263, 15])\n",
      "torch.Size([272, 1024]) torch.Size([271, 12])\n",
      "torch.Size([246, 1024]) torch.Size([245, 15])\n",
      "torch.Size([298, 1024]) torch.Size([297, 21])\n",
      "torch.Size([278, 1024]) torch.Size([277, 16])\n",
      "torch.Size([426, 1024]) torch.Size([425, 17])\n",
      "torch.Size([340, 1024]) torch.Size([339, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 820/1560 [00:08<00:06, 107.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([298, 1024]) torch.Size([297, 19])\n",
      "torch.Size([366, 1024]) torch.Size([365, 16])\n",
      "torch.Size([340, 1024]) torch.Size([339, 16])\n",
      "torch.Size([400, 1024]) torch.Size([399, 14])\n",
      "torch.Size([298, 1024]) torch.Size([297, 14])\n",
      "torch.Size([298, 1024]) torch.Size([297, 14])\n",
      "torch.Size([432, 1024]) torch.Size([431, 21])\n",
      "torch.Size([520, 1024]) torch.Size([519, 21])\n",
      "torch.Size([332, 1024]) torch.Size([331, 12])\n",
      "torch.Size([272, 1024]) torch.Size([271, 15])\n",
      "torch.Size([264, 1024]) torch.Size([263, 17])\n",
      "torch.Size([380, 1024]) torch.Size([379, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 842/1560 [00:08<00:06, 105.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([392, 1024]) torch.Size([391, 18])\n",
      "torch.Size([366, 1024]) torch.Size([365, 17])\n",
      "torch.Size([246, 1024]) torch.Size([245, 20])\n",
      "torch.Size([238, 1024]) torch.Size([237, 20])\n",
      "torch.Size([648, 1024]) torch.Size([647, 43])\n",
      "torch.Size([374, 1024]) torch.Size([373, 18])\n",
      "torch.Size([340, 1024]) torch.Size([339, 21])\n",
      "torch.Size([238, 1024]) torch.Size([237, 18])\n",
      "torch.Size([630, 1024]) torch.Size([629, 30])\n",
      "torch.Size([332, 1024]) torch.Size([331, 29])\n",
      "torch.Size([298, 1024]) torch.Size([297, 13])\n",
      "torch.Size([306, 1024]) torch.Size([305, 12])\n",
      "torch.Size([426, 1024]) torch.Size([425, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 864/1560 [00:08<00:06, 104.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([298, 1024]) torch.Size([297, 15])\n",
      "torch.Size([298, 1024]) torch.Size([297, 15])\n",
      "torch.Size([272, 1024]) torch.Size([271, 15])\n",
      "torch.Size([886, 1024]) torch.Size([885, 42])\n",
      "torch.Size([238, 1024]) torch.Size([237, 12])\n",
      "torch.Size([366, 1024]) torch.Size([365, 12])\n",
      "torch.Size([366, 1024]) torch.Size([365, 12])\n",
      "torch.Size([528, 1024]) torch.Size([527, 32])\n",
      "torch.Size([426, 1024]) torch.Size([425, 16])\n",
      "torch.Size([596, 1024]) torch.Size([595, 32])\n",
      "torch.Size([238, 1024]) torch.Size([237, 11])\n",
      "torch.Size([630, 1024]) torch.Size([629, 16])\n",
      "torch.Size([374, 1024]) torch.Size([373, 17])\n",
      "torch.Size([366, 1024]) torch.Size([365, 16])\n",
      "torch.Size([400, 1024]) torch.Size([399, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 888/1560 [00:08<00:06, 108.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([212, 1024]) torch.Size([211, 12])\n",
      "torch.Size([340, 1024]) torch.Size([339, 23])\n",
      "torch.Size([250, 1024]) torch.Size([249, 16])\n",
      "torch.Size([380, 1024]) torch.Size([379, 20])\n",
      "torch.Size([246, 1024]) torch.Size([245, 13])\n",
      "torch.Size([298, 1024]) torch.Size([297, 13])\n",
      "torch.Size([332, 1024]) torch.Size([331, 13])\n",
      "torch.Size([374, 1024]) torch.Size([373, 21])\n",
      "torch.Size([340, 1024]) torch.Size([339, 16])\n",
      "torch.Size([246, 1024]) torch.Size([245, 12])\n",
      "torch.Size([264, 1024]) torch.Size([263, 12])\n",
      "torch.Size([246, 1024]) torch.Size([245, 12])\n",
      "torch.Size([272, 1024]) torch.Size([271, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 911/1560 [00:09<00:05, 110.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([204, 1024]) torch.Size([203, 12])\n",
      "torch.Size([264, 1024]) torch.Size([263, 13])\n",
      "torch.Size([272, 1024]) torch.Size([271, 12])\n",
      "torch.Size([246, 1024]) torch.Size([245, 12])\n",
      "torch.Size([340, 1024]) torch.Size([339, 20])\n",
      "torch.Size([204, 1024]) torch.Size([203, 12])\n",
      "torch.Size([264, 1024]) torch.Size([263, 12])\n",
      "torch.Size([178, 1024]) torch.Size([177, 13])\n",
      "torch.Size([400, 1024]) torch.Size([399, 20])\n",
      "torch.Size([204, 1024]) torch.Size([203, 17])\n",
      "torch.Size([648, 1024]) torch.Size([647, 30])\n",
      "torch.Size([246, 1024]) torch.Size([245, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 935/1560 [00:09<00:05, 108.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([366, 1024]) torch.Size([365, 16])\n",
      "torch.Size([306, 1024]) torch.Size([305, 13])\n",
      "torch.Size([264, 1024]) torch.Size([263, 13])\n",
      "torch.Size([374, 1024]) torch.Size([373, 19])\n",
      "torch.Size([250, 1024]) torch.Size([249, 17])\n",
      "torch.Size([222, 1024]) torch.Size([221, 15])\n",
      "torch.Size([194, 1024]) torch.Size([193, 13])\n",
      "torch.Size([594, 1024]) torch.Size([593, 30])\n",
      "torch.Size([332, 1024]) torch.Size([331, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 958/1560 [00:09<00:05, 109.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([272, 1024]) torch.Size([271, 13])\n",
      "torch.Size([392, 1024]) torch.Size([391, 16])\n",
      "torch.Size([554, 1024]) torch.Size([553, 15])\n",
      "torch.Size([392, 1024]) torch.Size([391, 21])\n",
      "torch.Size([340, 1024]) torch.Size([339, 15])\n",
      "torch.Size([306, 1024]) torch.Size([305, 13])\n",
      "torch.Size([502, 1024]) torch.Size([501, 19])\n",
      "torch.Size([298, 1024]) torch.Size([297, 16])\n",
      "torch.Size([264, 1024]) torch.Size([263, 13])\n",
      "torch.Size([362, 1024]) torch.Size([361, 12])\n",
      "torch.Size([298, 1024]) torch.Size([297, 13])\n",
      "torch.Size([344, 1024]) torch.Size([343, 12])\n",
      "torch.Size([300, 1024]) torch.Size([299, 12])\n",
      "torch.Size([306, 1024]) torch.Size([305, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 980/1560 [00:09<00:05, 103.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([264, 1024]) torch.Size([263, 14])\n",
      "torch.Size([264, 1024]) torch.Size([263, 18])\n",
      "torch.Size([374, 1024]) torch.Size([373, 14])\n",
      "torch.Size([520, 1024]) torch.Size([519, 14])\n",
      "torch.Size([538, 1024]) torch.Size([537, 30])\n",
      "torch.Size([324, 1024]) torch.Size([323, 18])\n",
      "torch.Size([392, 1024]) torch.Size([391, 13])\n",
      "torch.Size([392, 1024]) torch.Size([391, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 1002/1560 [00:09<00:05, 102.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([392, 1024]) torch.Size([391, 13])\n",
      "torch.Size([392, 1024]) torch.Size([391, 16])\n",
      "torch.Size([366, 1024]) torch.Size([365, 29])\n",
      "torch.Size([494, 1024]) torch.Size([493, 21])\n",
      "torch.Size([460, 1024]) torch.Size([459, 17])\n",
      "torch.Size([400, 1024]) torch.Size([399, 16])\n",
      "torch.Size([360, 1024]) torch.Size([359, 14])\n",
      "torch.Size([682, 1024]) torch.Size([681, 16])\n",
      "torch.Size([392, 1024]) torch.Size([391, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 1026/1560 [00:10<00:04, 107.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([264, 1024]) torch.Size([263, 16])\n",
      "torch.Size([194, 1024]) torch.Size([193, 12])\n",
      "torch.Size([400, 1024]) torch.Size([399, 15])\n",
      "torch.Size([298, 1024]) torch.Size([297, 13])\n",
      "torch.Size([298, 1024]) torch.Size([297, 19])\n",
      "torch.Size([238, 1024]) torch.Size([237, 16])\n",
      "torch.Size([238, 1024]) torch.Size([237, 13])\n",
      "torch.Size([400, 1024]) torch.Size([399, 14])\n",
      "torch.Size([366, 1024]) torch.Size([365, 15])\n",
      "torch.Size([306, 1024]) torch.Size([305, 14])\n",
      "torch.Size([332, 1024]) torch.Size([331, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 1037/1560 [00:10<00:04, 105.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([434, 1024]) torch.Size([433, 20])\n",
      "torch.Size([426, 1024]) torch.Size([425, 20])\n",
      "torch.Size([434, 1024]) torch.Size([433, 22])\n",
      "torch.Size([296, 1024]) torch.Size([295, 16])\n",
      "torch.Size([278, 1024]) torch.Size([277, 14])\n",
      "torch.Size([170, 1024]) torch.Size([169, 13])\n",
      "torch.Size([554, 1024]) torch.Size([553, 13])\n",
      "torch.Size([588, 1024]) torch.Size([587, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 1060/1560 [00:10<00:04, 108.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([460, 1024]) torch.Size([459, 20])\n",
      "torch.Size([268, 1024]) torch.Size([267, 15])\n",
      "torch.Size([240, 1024]) torch.Size([239, 13])\n",
      "torch.Size([268, 1024]) torch.Size([267, 15])\n",
      "torch.Size([268, 1024]) torch.Size([267, 16])\n",
      "torch.Size([268, 1024]) torch.Size([267, 13])\n",
      "torch.Size([324, 1024]) torch.Size([323, 19])\n",
      "torch.Size([454, 1024]) torch.Size([453, 19])\n",
      "torch.Size([240, 1024]) torch.Size([239, 15])\n",
      "torch.Size([264, 1024]) torch.Size([263, 14])\n",
      "torch.Size([272, 1024]) torch.Size([271, 18])\n",
      "torch.Size([306, 1024]) torch.Size([305, 18])\n",
      "torch.Size([366, 1024]) torch.Size([365, 19])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 1083/1560 [00:10<00:04, 105.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([912, 1024]) torch.Size([911, 43])\n",
      "torch.Size([264, 1024]) torch.Size([263, 13])\n",
      "torch.Size([306, 1024]) torch.Size([305, 12])\n",
      "torch.Size([426, 1024]) torch.Size([425, 28])\n",
      "torch.Size([374, 1024]) torch.Size([373, 12])\n",
      "torch.Size([298, 1024]) torch.Size([297, 15])\n",
      "torch.Size([340, 1024]) torch.Size([339, 18])\n",
      "torch.Size([366, 1024]) torch.Size([365, 16])\n",
      "torch.Size([374, 1024]) torch.Size([373, 13])\n",
      "torch.Size([272, 1024]) torch.Size([271, 15])\n",
      "torch.Size([264, 1024]) torch.Size([263, 15])\n",
      "torch.Size([246, 1024]) torch.Size([245, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1106/1560 [00:10<00:04, 106.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([306, 1024]) torch.Size([305, 14])\n",
      "torch.Size([272, 1024]) torch.Size([271, 16])\n",
      "torch.Size([306, 1024]) torch.Size([305, 16])\n",
      "torch.Size([494, 1024]) torch.Size([493, 16])\n",
      "torch.Size([238, 1024]) torch.Size([237, 13])\n",
      "torch.Size([204, 1024]) torch.Size([203, 12])\n",
      "torch.Size([170, 1024]) torch.Size([169, 13])\n",
      "torch.Size([136, 1024]) torch.Size([135, 12])\n",
      "torch.Size([554, 1024]) torch.Size([553, 21])\n",
      "torch.Size([630, 1024]) torch.Size([629, 21])\n",
      "torch.Size([502, 1024]) torch.Size([501, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1129/1560 [00:11<00:04, 104.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([272, 1024]) torch.Size([271, 21])\n",
      "torch.Size([332, 1024]) torch.Size([331, 18])\n",
      "torch.Size([228, 1024]) torch.Size([227, 18])\n",
      "torch.Size([446, 1024]) torch.Size([445, 17])\n",
      "torch.Size([400, 1024]) torch.Size([399, 16])\n",
      "torch.Size([332, 1024]) torch.Size([331, 16])\n",
      "torch.Size([374, 1024]) torch.Size([373, 15])\n",
      "torch.Size([306, 1024]) torch.Size([305, 14])\n",
      "torch.Size([340, 1024]) torch.Size([339, 14])\n",
      "torch.Size([392, 1024]) torch.Size([391, 15])\n",
      "torch.Size([980, 1024]) torch.Size([979, 57])\n",
      "torch.Size([436, 1024]) torch.Size([435, 24])\n",
      "torch.Size([306, 1024]) torch.Size([305, 16])\n",
      "torch.Size([264, 1024]) torch.Size([263, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 1151/1560 [00:11<00:03, 104.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([374, 1024]) torch.Size([373, 20])\n",
      "torch.Size([332, 1024]) torch.Size([331, 12])\n",
      "torch.Size([340, 1024]) torch.Size([339, 20])\n",
      "torch.Size([306, 1024]) torch.Size([305, 20])\n",
      "torch.Size([366, 1024]) torch.Size([365, 20])\n",
      "torch.Size([246, 1024]) torch.Size([245, 20])\n",
      "torch.Size([170, 1024]) torch.Size([169, 12])\n",
      "torch.Size([204, 1024]) torch.Size([203, 13])\n",
      "torch.Size([238, 1024]) torch.Size([237, 12])\n",
      "torch.Size([264, 1024]) torch.Size([263, 12])\n",
      "torch.Size([204, 1024]) torch.Size([203, 13])\n",
      "torch.Size([170, 1024]) torch.Size([169, 12])\n",
      "torch.Size([204, 1024]) torch.Size([203, 13])\n",
      "torch.Size([264, 1024]) torch.Size([263, 12])\n",
      "torch.Size([332, 1024]) torch.Size([331, 13])\n",
      "torch.Size([332, 1024]) torch.Size([331, 12])\n",
      "torch.Size([366, 1024]) torch.Size([365, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 1176/1560 [00:11<00:03, 103.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([306, 1024]) torch.Size([305, 12])\n",
      "torch.Size([468, 1024]) torch.Size([467, 17])\n",
      "torch.Size([622, 1024]) torch.Size([621, 12])\n",
      "torch.Size([434, 1024]) torch.Size([433, 17])\n",
      "torch.Size([528, 1024]) torch.Size([527, 21])\n",
      "torch.Size([588, 1024]) torch.Size([587, 29])\n",
      "torch.Size([750, 1024]) torch.Size([749, 29])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1198/1560 [00:11<00:03, 101.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([296, 1024]) torch.Size([295, 18])\n",
      "torch.Size([380, 1024]) torch.Size([379, 18])\n",
      "torch.Size([750, 1024]) torch.Size([749, 32])\n",
      "torch.Size([298, 1024]) torch.Size([297, 13])\n",
      "torch.Size([374, 1024]) torch.Size([373, 13])\n",
      "torch.Size([392, 1024]) torch.Size([391, 12])\n",
      "torch.Size([366, 1024]) torch.Size([365, 13])\n",
      "torch.Size([332, 1024]) torch.Size([331, 13])\n",
      "torch.Size([272, 1024]) torch.Size([271, 12])\n",
      "torch.Size([332, 1024]) torch.Size([331, 12])\n",
      "torch.Size([298, 1024]) torch.Size([297, 20])\n",
      "torch.Size([366, 1024]) torch.Size([365, 19])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1223/1560 [00:12<00:03, 107.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([340, 1024]) torch.Size([339, 17])\n",
      "torch.Size([272, 1024]) torch.Size([271, 12])\n",
      "torch.Size([264, 1024]) torch.Size([263, 12])\n",
      "torch.Size([426, 1024]) torch.Size([425, 18])\n",
      "torch.Size([306, 1024]) torch.Size([305, 13])\n",
      "torch.Size([298, 1024]) torch.Size([297, 14])\n",
      "torch.Size([306, 1024]) torch.Size([305, 14])\n",
      "torch.Size([246, 1024]) torch.Size([245, 13])\n",
      "torch.Size([352, 1024]) torch.Size([351, 15])\n",
      "torch.Size([268, 1024]) torch.Size([267, 13])\n",
      "torch.Size([250, 1024]) torch.Size([249, 15])\n",
      "torch.Size([296, 1024]) torch.Size([295, 16])\n",
      "torch.Size([306, 1024]) torch.Size([305, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 1247/1560 [00:12<00:02, 111.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([324, 1024]) torch.Size([323, 19])\n",
      "torch.Size([296, 1024]) torch.Size([295, 16])\n",
      "torch.Size([278, 1024]) torch.Size([277, 14])\n",
      "torch.Size([250, 1024]) torch.Size([249, 16])\n",
      "torch.Size([268, 1024]) torch.Size([267, 14])\n",
      "torch.Size([222, 1024]) torch.Size([221, 16])\n",
      "torch.Size([212, 1024]) torch.Size([211, 14])\n",
      "torch.Size([422, 1024]) torch.Size([421, 15])\n",
      "torch.Size([384, 1024]) torch.Size([383, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 1270/1560 [00:12<00:02, 107.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([340, 1024]) torch.Size([339, 17])\n",
      "torch.Size([784, 1024]) torch.Size([783, 17])\n",
      "torch.Size([630, 1024]) torch.Size([629, 16])\n",
      "torch.Size([426, 1024]) torch.Size([425, 19])\n",
      "torch.Size([488, 1024]) torch.Size([487, 24])\n",
      "torch.Size([246, 1024]) torch.Size([245, 16])\n",
      "torch.Size([246, 1024]) torch.Size([245, 16])\n",
      "torch.Size([246, 1024]) torch.Size([245, 16])\n",
      "torch.Size([332, 1024]) torch.Size([331, 16])\n",
      "torch.Size([502, 1024]) torch.Size([501, 24])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1293/1560 [00:12<00:02, 108.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([238, 1024]) torch.Size([237, 12])\n",
      "torch.Size([212, 1024]) torch.Size([211, 13])\n",
      "torch.Size([434, 1024]) torch.Size([433, 28])\n",
      "torch.Size([210, 1024]) torch.Size([209, 17])\n",
      "torch.Size([376, 1024]) torch.Size([375, 30])\n",
      "torch.Size([280, 1024]) torch.Size([279, 29])\n",
      "torch.Size([416, 1024]) torch.Size([415, 16])\n",
      "torch.Size([298, 1024]) torch.Size([297, 15])\n",
      "torch.Size([340, 1024]) torch.Size([339, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 1315/1560 [00:12<00:02, 98.81it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([528, 1024]) torch.Size([527, 16])\n",
      "torch.Size([852, 1024]) torch.Size([851, 43])\n",
      "torch.Size([400, 1024]) torch.Size([399, 14])\n",
      "torch.Size([400, 1024]) torch.Size([399, 20])\n",
      "torch.Size([810, 1024]) torch.Size([809, 43])\n",
      "torch.Size([434, 1024]) torch.Size([433, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 1338/1560 [00:13<00:02, 101.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([332, 1024]) torch.Size([331, 17])\n",
      "torch.Size([238, 1024]) torch.Size([237, 17])\n",
      "torch.Size([306, 1024]) torch.Size([305, 15])\n",
      "torch.Size([268, 1024]) torch.Size([267, 14])\n",
      "torch.Size([340, 1024]) torch.Size([339, 16])\n",
      "torch.Size([264, 1024]) torch.Size([263, 13])\n",
      "torch.Size([426, 1024]) torch.Size([425, 16])\n",
      "torch.Size([340, 1024]) torch.Size([339, 13])\n",
      "torch.Size([400, 1024]) torch.Size([399, 15])\n",
      "torch.Size([332, 1024]) torch.Size([331, 13])\n",
      "torch.Size([306, 1024]) torch.Size([305, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 1361/1560 [00:13<00:01, 99.67it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([306, 1024]) torch.Size([305, 17])\n",
      "torch.Size([332, 1024]) torch.Size([331, 15])\n",
      "torch.Size([238, 1024]) torch.Size([237, 13])\n",
      "torch.Size([212, 1024]) torch.Size([211, 12])\n",
      "torch.Size([264, 1024]) torch.Size([263, 13])\n",
      "torch.Size([366, 1024]) torch.Size([365, 16])\n",
      "torch.Size([340, 1024]) torch.Size([339, 18])\n",
      "torch.Size([238, 1024]) torch.Size([237, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 1383/1560 [00:13<00:01, 102.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([426, 1024]) torch.Size([425, 16])\n",
      "torch.Size([332, 1024]) torch.Size([331, 14])\n",
      "torch.Size([374, 1024]) torch.Size([373, 17])\n",
      "torch.Size([298, 1024]) torch.Size([297, 16])\n",
      "torch.Size([306, 1024]) torch.Size([305, 16])\n",
      "torch.Size([426, 1024]) torch.Size([425, 16])\n",
      "torch.Size([554, 1024]) torch.Size([553, 13])\n",
      "torch.Size([332, 1024]) torch.Size([331, 20])\n",
      "torch.Size([306, 1024]) torch.Size([305, 20])\n",
      "torch.Size([246, 1024]) torch.Size([245, 21])\n",
      "torch.Size([246, 1024]) torch.Size([245, 20])\n",
      "torch.Size([460, 1024]) torch.Size([459, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1405/1560 [00:13<00:01, 101.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([522, 1024]) torch.Size([521, 20])\n",
      "torch.Size([594, 1024]) torch.Size([593, 20])\n",
      "torch.Size([526, 1024]) torch.Size([525, 20])\n",
      "torch.Size([222, 1024]) torch.Size([221, 15])\n",
      "torch.Size([434, 1024]) torch.Size([433, 16])\n",
      "torch.Size([434, 1024]) torch.Size([433, 12])\n",
      "torch.Size([366, 1024]) torch.Size([365, 16])\n",
      "torch.Size([426, 1024]) torch.Size([425, 22])\n",
      "torch.Size([332, 1024]) torch.Size([331, 13])\n",
      "torch.Size([374, 1024]) torch.Size([373, 12])\n",
      "torch.Size([426, 1024]) torch.Size([425, 12])\n",
      "torch.Size([392, 1024]) torch.Size([391, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1428/1560 [00:13<00:01, 106.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([246, 1024]) torch.Size([245, 12])\n",
      "torch.Size([724, 1024]) torch.Size([723, 21])\n",
      "torch.Size([246, 1024]) torch.Size([245, 15])\n",
      "torch.Size([238, 1024]) torch.Size([237, 16])\n",
      "torch.Size([212, 1024]) torch.Size([211, 16])\n",
      "torch.Size([238, 1024]) torch.Size([237, 13])\n",
      "torch.Size([204, 1024]) torch.Size([203, 12])\n",
      "torch.Size([332, 1024]) torch.Size([331, 15])\n",
      "torch.Size([374, 1024]) torch.Size([373, 17])\n",
      "torch.Size([468, 1024]) torch.Size([467, 21])\n",
      "torch.Size([374, 1024]) torch.Size([373, 17])\n",
      "torch.Size([468, 1024]) torch.Size([467, 21])\n",
      "torch.Size([340, 1024]) torch.Size([339, 16])\n",
      "torch.Size([340, 1024]) torch.Size([339, 16])\n",
      "torch.Size([562, 1024]) torch.Size([561, 32])\n",
      "torch.Size([238, 1024]) torch.Size([237, 12])\n",
      "torch.Size([264, 1024]) torch.Size([263, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 1450/1560 [00:14<00:01, 107.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([332, 1024]) torch.Size([331, 18])\n",
      "torch.Size([340, 1024]) torch.Size([339, 18])\n",
      "torch.Size([306, 1024]) torch.Size([305, 15])\n",
      "torch.Size([264, 1024]) torch.Size([263, 12])\n",
      "torch.Size([238, 1024]) torch.Size([237, 15])\n",
      "torch.Size([340, 1024]) torch.Size([339, 18])\n",
      "torch.Size([332, 1024]) torch.Size([331, 13])\n",
      "torch.Size([268, 1024]) torch.Size([267, 16])\n",
      "torch.Size([398, 1024]) torch.Size([397, 28])\n",
      "torch.Size([306, 1024]) torch.Size([305, 20])\n",
      "torch.Size([648, 1024]) torch.Size([647, 13])\n",
      "torch.Size([562, 1024]) torch.Size([561, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 1473/1560 [00:14<00:00, 107.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([690, 1024]) torch.Size([689, 12])\n",
      "torch.Size([426, 1024]) torch.Size([425, 20])\n",
      "torch.Size([434, 1024]) torch.Size([433, 20])\n",
      "torch.Size([332, 1024]) torch.Size([331, 18])\n",
      "torch.Size([306, 1024]) torch.Size([305, 18])\n",
      "torch.Size([366, 1024]) torch.Size([365, 18])\n",
      "torch.Size([264, 1024]) torch.Size([263, 18])\n",
      "torch.Size([246, 1024]) torch.Size([245, 18])\n",
      "torch.Size([212, 1024]) torch.Size([211, 18])\n",
      "torch.Size([238, 1024]) torch.Size([237, 18])\n",
      "torch.Size([298, 1024]) torch.Size([297, 18])\n",
      "torch.Size([306, 1024]) torch.Size([305, 18])\n",
      "torch.Size([426, 1024]) torch.Size([425, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1496/1560 [00:14<00:00, 104.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([298, 1024]) torch.Size([297, 12])\n",
      "torch.Size([374, 1024]) torch.Size([373, 19])\n",
      "torch.Size([306, 1024]) torch.Size([305, 16])\n",
      "torch.Size([238, 1024]) torch.Size([237, 16])\n",
      "torch.Size([272, 1024]) torch.Size([271, 17])\n",
      "torch.Size([272, 1024]) torch.Size([271, 16])\n",
      "torch.Size([272, 1024]) torch.Size([271, 17])\n",
      "torch.Size([298, 1024]) torch.Size([297, 16])\n",
      "torch.Size([298, 1024]) torch.Size([297, 16])\n",
      "torch.Size([306, 1024]) torch.Size([305, 16])\n",
      "torch.Size([238, 1024]) torch.Size([237, 16])\n",
      "torch.Size([298, 1024]) torch.Size([297, 17])\n",
      "torch.Size([272, 1024]) torch.Size([271, 17])\n",
      "torch.Size([340, 1024]) torch.Size([339, 13])\n",
      "torch.Size([264, 1024]) torch.Size([263, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 1519/1560 [00:14<00:00, 106.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([332, 1024]) torch.Size([331, 13])\n",
      "torch.Size([306, 1024]) torch.Size([305, 12])\n",
      "torch.Size([596, 1024]) torch.Size([595, 43])\n",
      "torch.Size([528, 1024]) torch.Size([527, 43])\n",
      "torch.Size([656, 1024]) torch.Size([655, 16])\n",
      "torch.Size([264, 1024]) torch.Size([263, 12])\n",
      "torch.Size([264, 1024]) torch.Size([263, 12])\n",
      "torch.Size([238, 1024]) torch.Size([237, 12])\n",
      "torch.Size([306, 1024]) torch.Size([305, 13])\n",
      "torch.Size([264, 1024]) torch.Size([263, 14])\n",
      "torch.Size([332, 1024]) torch.Size([331, 12])\n",
      "torch.Size([306, 1024]) torch.Size([305, 13])\n",
      "torch.Size([306, 1024]) torch.Size([305, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1544/1560 [00:15<00:00, 106.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([238, 1024]) torch.Size([237, 12])\n",
      "torch.Size([204, 1024]) torch.Size([203, 13])\n",
      "torch.Size([296, 1024]) torch.Size([295, 14])\n",
      "torch.Size([296, 1024]) torch.Size([295, 14])\n",
      "torch.Size([554, 1024]) torch.Size([553, 16])\n",
      "torch.Size([460, 1024]) torch.Size([459, 16])\n",
      "torch.Size([494, 1024]) torch.Size([493, 15])\n",
      "torch.Size([468, 1024]) torch.Size([467, 16])\n",
      "torch.Size([332, 1024]) torch.Size([331, 15])\n",
      "torch.Size([374, 1024]) torch.Size([373, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1560/1560 [00:15<00:00, 102.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([298, 1024]) torch.Size([297, 16])\n",
      "torch.Size([298, 1024]) torch.Size([297, 15])\n",
      "torch.Size([400, 1024]) torch.Size([399, 29])\n",
      "torch.Size([426, 1024]) torch.Size([425, 12])\n",
      "torch.Size([460, 1024]) torch.Size([459, 12])\n",
      "torch.Size([426, 1024]) torch.Size([425, 15])\n",
      "torch.Size([392, 1024]) torch.Size([391, 14])\n"
     ]
    }
   ],
   "source": [
    "wav_dir = \"/data/codes/prep_ps_pykaldi/prep_data/wav\"\n",
    "\n",
    "hubert_features = []\n",
    "for index in tqdm(metadata.index):\n",
    "    wav_id = metadata[\"id\"][index]\n",
    "    alignment = metadata[\"alignment\"][index]\n",
    "\n",
    "    alignment = json.loads(alignment)\n",
    "    wav, sr = librosa.load(f'{wav_dir}/{wav_id}.wav', sr=16000)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = processor(wav, return_tensors=\"pt\", sampling_rate=16000)\n",
    "        features = model(features[\"input_values\"].cuda()).last_hidden_state\n",
    "\n",
    "    index = torch.arange(features.shape[1]).unsqueeze(-1).cuda()\n",
    "    expanded_index = index.expand((-1, 2)).flatten()\n",
    "    features = features[0][expanded_index]\n",
    "\n",
    "    features, phonemes = extract_feature(alignment, features)\n",
    "\n",
    "    features = torch.concat([features, torch.zeros(MAX_LENGTH-len(phonemes), 1024)], axis=0)\n",
    "    hubert_features.append(features)\n",
    "\n",
    "hubert_features = torch.stack(hubert_features, dim=0)\n",
    "hubert_features = hubert_features.numpy()\n",
    "# np.save(f'{out_dir}/hubert_features.npy', hubert_features)\n",
    "hubert_features = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Wav2vec Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuyendv/miniconda3/envs/ps/lib/python3.8/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2Model.from_pretrained(\"/data/codes/prep_ps_pykaldi/pretrained/charsiu\").eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(alignment, features):\n",
    "    index = 0\n",
    "    phonemes = []\n",
    "    indices = -1 * torch.ones(alignment[-1][1] + alignment[-1][2])\n",
    "    for phoneme, start_frame, duration in alignment:\n",
    "        if phoneme == \"SIL\":\n",
    "            continue\n",
    "        end_frame = start_frame + duration\n",
    "        indices[start_frame:end_frame] = index\n",
    "        phonemes.append(phoneme)\n",
    "        index += 1\n",
    "\n",
    "    indices[indices==-1] = indices.max() + 1\n",
    "\n",
    "    indices = torch.nn.functional.one_hot(indices.long(), num_classes=int(indices.max().item())+1).cuda()\n",
    "    indices = indices / indices.sum(0, keepdim=True)\n",
    "    \n",
    "    if features.shape[0] != indices.shape[0]:\n",
    "        print(features.shape[0], indices.shape[0])\n",
    "        features = features[0:indices.shape[0]]\n",
    "        print(\"Hello\")\n",
    "    features = torch.matmul(indices.transpose(0, 1), features)\n",
    "\n",
    "    return features[:-1].cpu(), phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1560/1560 [00:09<00:00, 166.98it/s]\n"
     ]
    }
   ],
   "source": [
    "wav_dir = \"/data/codes/prep_ps_pykaldi/prep_data/wav\"\n",
    "\n",
    "hubert_features = []\n",
    "for index in tqdm(metadata.index):\n",
    "    wav_id = metadata[\"id\"][index]\n",
    "    alignment = metadata[\"alignment\"][index]\n",
    "\n",
    "    alignment = json.loads(alignment)\n",
    "    wav, sr = librosa.load(f'{wav_dir}/{wav_id}.wav', sr=16000)\n",
    "    features = torch.from_numpy(wav).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        # features = processor(wav, return_tensors=\"pt\", sampling_rate=16000)\n",
    "        features = model(features).last_hidden_state\n",
    "        if index % 100 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    features, phonemes = extract_feature(alignment, features[0])\n",
    "\n",
    "    features = torch.concat([features, torch.zeros(MAX_LENGTH-len(phonemes), 768)], axis=0)\n",
    "    hubert_features.append(features)\n",
    "\n",
    "hubert_features = torch.stack(hubert_features, dim=0)\n",
    "hubert_features = hubert_features.numpy()\n",
    "# np.save(f'{out_dir}/wav2vec_features.npy', hubert_features)\n",
    "hubert_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
