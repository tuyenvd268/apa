{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /data/codes/prep_ps_pykaldi/\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [json.loads(line.strip()) for line in lines]\n",
    "    \n",
    "    return lines\n",
    "\n",
    "path = \"prep_data/info_in_domain_short_sentence_testset.jsonl\"\n",
    "metadata = load_jsonl(path)\n",
    "metadata = pd.DataFrame(metadata)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/codes/prep_ps_pykaldi/exp/sm/test/merged_gop.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "metadata = metadata[metadata.id.isin(data)]\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gop_feature(id):\n",
    "    sample = data[str(id)]\n",
    "    features = [\n",
    "        np.array(feature) for feature, phoneme in zip(sample[\"gopt\"], sample[\"phones\"][0])\n",
    "        if phoneme != \"SIL\"\n",
    "    ]\n",
    "    return np.stack(features)\n",
    "\n",
    "def extract_phonemes(id):\n",
    "    sample = data[str(id)]\n",
    "    phonemes = [\n",
    "        re.sub(\"\\d\", \"\",phoneme.split(\"_\")[0]) for phoneme in sample[\"phones\"][0]\n",
    "        if phoneme != \"SIL\"\n",
    "    ]\n",
    "    return phonemes\n",
    "\n",
    "metadata[\"features\"] = metadata.id.apply(lambda x: extract_gop_feature(x))\n",
    "metadata[\"kaldi_phoneme\"] = metadata.id.apply(lambda x: extract_phonemes(x))\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_path = \"/data/codes/prep_ps_pykaldi/exp/sm/test/merged_align.out\"\n",
    "align_df = pd.read_csv(align_path, names=[\"id\", \"alignment\"], sep=\"\\t\")\n",
    "\n",
    "def extract_duration(alignment):\n",
    "    alignment = json.loads(alignment)\n",
    "    durations = []\n",
    "    \n",
    "    for phoneme, start, duration in alignment:\n",
    "        if phoneme == \"SIL\":\n",
    "            continue\n",
    "        durations.append(round(duration * 0.02, 4))\n",
    "\n",
    "    return durations\n",
    "\n",
    "def extract_phonemes(alignment):\n",
    "    alignment = json.loads(alignment)\n",
    "    phonemes = []\n",
    "    \n",
    "    for phoneme, start, duration in alignment:\n",
    "        if phoneme == \"SIL\":\n",
    "            continue\n",
    "        phonemes.append(phoneme.split(\"_\")[0])\n",
    "\n",
    "    return phonemes\n",
    "\n",
    "align_df[\"durations\"] = align_df[\"alignment\"].apply(lambda x: extract_duration(x))\n",
    "align_df[\"phonemes\"] = align_df[\"alignment\"].apply(lambda x: extract_phonemes(x))\n",
    "align_df[\"id\"] = align_df[\"id\"].apply(str)\n",
    "align_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.merge(metadata, align_df[[\"id\", \"durations\"]], how=\"left\", on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "features = metadata[\"features\"].to_list()\n",
    "features = np.concatenate(features)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features)\n",
    "\n",
    "with open('resources/scaler.pkl','wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phone_dict = []\n",
    "# for word in metadata[\"arpas\"].to_list():\n",
    "#     phone_dict += word\n",
    "# phone_dict = list(set(phone_dict))\n",
    "# phone_dict.sort()\n",
    "\n",
    "# phone_dict.insert(0, \"PAD\")\n",
    "# vocab= {}\n",
    "# for key in phone_dict:\n",
    "#     key = re.sub(\"\\d\", \"\", key)\n",
    "#     if key not in vocab:\n",
    "#         vocab[key] = len(vocab)\n",
    "# phone_dict = {re.sub(\"\\d\", \"\", key):value for value, key in enumerate(phone_dict)}\n",
    "\n",
    "# with open(\"resources/phone_dict.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json_obj = json.dumps(vocab, indent=4, ensure_ascii=False)\n",
    "#     f.write(json_obj)\n",
    "\n",
    "with open(\"resources/phone_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    phone_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['phones'] = metadata[\"arpas\"].apply(lambda word: [phone_dict[re.sub(\"\\d\", \"\", x)] for x in word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class PrepDataset(Dataset):\n",
    "    def __init__(self, metadata,):\n",
    "        self.metadata = metadata\n",
    "        self.scaler = pickle.load(open('resources/scaler.pkl','rb'))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.metadata.shape[0]\n",
    "    \n",
    "    def parse_data(self, features, phones, phone_scores, durations):\n",
    "        features = torch.tensor(features)\n",
    "        durations = torch.tensor(durations)\n",
    "        phones = torch.tensor(phones)\n",
    "        phone_scores = torch.tensor(phone_scores) / 50\n",
    "\n",
    "        features = torch.concat([features, durations.unsqueeze(-1)], dim=-1)        \n",
    "        return {\n",
    "            \"features\": features,\n",
    "            \"phones\": phones,\n",
    "            \"phone_scores\":phone_scores\n",
    "        }\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        features = self.metadata[\"features\"][index]\n",
    "\n",
    "        features = self.scaler.transform(features)\n",
    "        phones = self.metadata[\"phones\"][index]\n",
    "        phone_scores = self.metadata[\"phone_scores\"][index]\n",
    "        durations =self.metadata[\"durations\"][index]\n",
    "\n",
    "        return self.parse_data(\n",
    "            features=features, \n",
    "            phones=phones, \n",
    "            phone_scores=phone_scores,\n",
    "            durations=durations\n",
    "        )\n",
    "        \n",
    "    def pad_1d(self, input_ids, pad_value=-1, max_length=64):\n",
    "        if max_length is None:\n",
    "            max_length = max([len(sample) for sample in input_ids])        \n",
    "            \n",
    "        attention_masks = []\n",
    "        for i in range(len(input_ids)):\n",
    "            if input_ids[i].size(0) < max_length:\n",
    "                input_ids[i] = torch.cat(\n",
    "                    (\n",
    "                        input_ids[i], \n",
    "                        torch.Tensor([pad_value, ]*(max_length-len(input_ids[i])))\n",
    "                        )\n",
    "                    )\n",
    "            elif input_ids[i].size(0) > max_length:\n",
    "                input_ids[i] = input_ids[i][0:max_length]\n",
    "                \n",
    "            attention_masks.append(input_ids[i] != pad_value)\n",
    "            \n",
    "        return {\n",
    "            \"input_ids\": torch.vstack(input_ids),\n",
    "            \"attention_mask\": torch.vstack(attention_masks)\n",
    "        }\n",
    "        \n",
    "    def pad_2d(self, inputs, pad_value=0, max_length=64):\n",
    "        # max_length = max([len(sample) for sample in inputs])        \n",
    "            \n",
    "        for i in range(len(inputs)):\n",
    "            if inputs[i].size(0) < max_length:\n",
    "                inputs[i] = torch.cat(\n",
    "                    (\n",
    "                        inputs[i], \n",
    "                        pad_value*torch.ones((max_length-len(inputs[i]), 83))\n",
    "                        )\n",
    "                    )\n",
    "            elif inputs[i].size(0) > max_length:\n",
    "                inputs[i] = inputs[i][0:max_length]\n",
    "                \n",
    "        return torch.stack(inputs, dim=0)\n",
    "        \n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        features = [sample[\"features\"] for sample in batch]\n",
    "        phone_scores = [sample[\"phone_scores\"] for sample in batch]\n",
    "        phones = [sample[\"phones\"] for sample in batch]\n",
    "        \n",
    "        outputs = self.pad_1d(phones, pad_value=0)\n",
    "        phones = outputs[\"input_ids\"]\n",
    "        phones_mask = outputs[\"attention_mask\"]\n",
    "        \n",
    "        outputs = self.pad_1d(phone_scores, pad_value=-1)\n",
    "        phone_scores = outputs[\"input_ids\"]\n",
    "        \n",
    "        features = self.pad_2d(features, pad_value=0)\n",
    "                     \n",
    "        return {\n",
    "            \"features\": features,\n",
    "            \"phones\": phones,\n",
    "            \"phone_scores\": phone_scores\n",
    "        }\n",
    "\n",
    "dataset = PrepDataset(metadata)\n",
    "dataloader = DataLoader(dataset, batch_size=8, collate_fn=dataset.collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    features = batch[\"features\"]\n",
    "    phones = batch[\"phones\"]\n",
    "    phone_scores = batch[\"phone_scores\"]\n",
    "    \n",
    "    print(features.shape)\n",
    "    print(phones.shape)\n",
    "    print(phone_scores.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def get_sinusoid_encoding(n_position, d_hid):\n",
    "    def get_position_angle_vec(position):\n",
    "        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
    "\n",
    "    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "\n",
    "    return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n",
    "\n",
    "\n",
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    def norm_cdf(x):\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "        tensor.erfinv_()\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        \n",
    "        return tensor\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class GOPT(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, depth, input_dim=84, max_length=50, num_phone=40):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(dim=embed_dim, num_heads=num_heads) \n",
    "                for i in range(depth)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, max_length+1, self.embed_dim))\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "\n",
    "        self.in_proj = nn.Linear(self.input_dim, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim * 2, embed_dim)\n",
    "        self.mlp_head_phn = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))\n",
    "\n",
    "        self.mlp_head_word= nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))\n",
    "\n",
    "        self.num_phone = num_phone\n",
    "        self.phn_proj = nn.Linear(num_phone, embed_dim)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.mlp_head_utt = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))\n",
    "\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "\n",
    "    def forward(self, x, phn):\n",
    "        B = x.shape[0]\n",
    "        phn_one_hot = torch.nn.functional.one_hot(phn.long()+1, num_classes=self.num_phone).float()\n",
    "        phn_embed = self.phn_proj(phn_one_hot)\n",
    "\n",
    "        if self.embed_dim != self.input_dim:\n",
    "            x = self.in_proj(x)\n",
    "\n",
    "        x = torch.cat([x, phn_embed], dim=-1)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        cls_token = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = x + self.pos_embed[:,:x.shape[1],:]\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        u = self.mlp_head_utt(x[:, 0])\n",
    "        p = self.mlp_head_phn(x[:, 1:])\n",
    "        w = self.mlp_head_word(x[:, 1:])\n",
    "        return u, p, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "# trainset, testset = train_test_split(metadata, test_size=0.5, random_state=42)\n",
    "index = 15000\n",
    "trainset = metadata[:index]\n",
    "testset = metadata[index:]\n",
    "\n",
    "trainset.reset_index(inplace=True)\n",
    "testset.reset_index(inplace=True)\n",
    "\n",
    "trainset = PrepDataset(trainset)\n",
    "trainloader = DataLoader(\n",
    "    dataset=trainset,\n",
    "    batch_size=8,\n",
    "    collate_fn=trainset.collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "testset = PrepDataset(testset)\n",
    "testloader = DataLoader(\n",
    "    dataset=testset,\n",
    "    batch_size=1,\n",
    "    collate_fn=testset.collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gopt_model = GOPT(\n",
    "    embed_dim=32, num_heads=1, \n",
    "    depth=3, input_dim=83, \n",
    "    max_length=128, num_phone=62).to(device)\n",
    "\n",
    "trainables = [p for p in gopt_model.parameters() if p.requires_grad]\n",
    "\n",
    "lr = 3e-4\n",
    "optimizer = torch.optim.Adam(\n",
    "    trainables, lr, weight_decay=5e-7, betas=(0.95, 0.999))\n",
    "\n",
    "scheduler = MultiStepLR(\n",
    "    optimizer, list(range(10, 100, 5)), gamma=0.5, last_epoch=-1)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_phn(audio_output, target):\n",
    "    valid_token_pred = []\n",
    "    valid_token_target = []\n",
    "    # audio_output = audio_output.squeeze(2)\n",
    "    for i in range(audio_output.shape[0]):\n",
    "        for j in range(audio_output.shape[1]):\n",
    "            # only count valid tokens, not padded tokens (represented by negative values)\n",
    "            if target[i, j] >= 0:\n",
    "                valid_token_pred.append(audio_output[i, j])\n",
    "                valid_token_target.append(target[i, j])\n",
    "    valid_token_target = np.array(valid_token_target)\n",
    "    valid_token_pred = np.array(valid_token_pred)\n",
    "\n",
    "    # valid_token_mse = np.mean((valid_token_target - valid_token_pred) ** 2)\n",
    "    valid_token_mse = np.mean(np.abs(valid_token_target - valid_token_pred))\n",
    "    corr = np.corrcoef(valid_token_pred, valid_token_target)[0, 1]\n",
    "    return valid_token_mse, corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(50):\n",
    "    gopt_model.train()\n",
    "    train_tqdm = tqdm(trainloader, \"Training\")\n",
    "\n",
    "    for batch in train_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        features = batch[\"features\"].to(device)\n",
    "        phones = batch[\"phones\"].to(device)\n",
    "        phone_labels = batch[\"phone_scores\"].to(device)\n",
    "\n",
    "        # warm_up_step = 100\n",
    "        # if global_step <= warm_up_step and global_step % 5 == 0:\n",
    "        #     warm_lr = (global_step / warm_up_step) * lr\n",
    "        #     for param_group in optimizer.param_groups:\n",
    "        #         param_group['lr'] = warm_lr\n",
    "\n",
    "        utterance_preds, phone_preds, word_preds = gopt_model(x=features.float(), phn=phones.long())\n",
    "        \n",
    "        mask = phone_labels >=0\n",
    "        phone_preds = phone_preds.squeeze(2)\n",
    "        phone_preds = phone_preds * mask\n",
    "        phone_labels = phone_labels * mask\n",
    "        \n",
    "        loss_phn = loss_fn(phone_preds, phone_labels)\n",
    "        loss_phn = loss_phn * (mask.shape[0] * mask.shape[1]) / torch.sum(mask)\n",
    "        \n",
    "        loss_phn.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if global_step > warm_up_step:\n",
    "        #     scheduler.step()\n",
    "        \n",
    "        global_step += 1\n",
    "\n",
    "        train_tqdm.set_postfix(loss_phn=loss_phn.item())\n",
    "    \n",
    "    A_phn, A_phn_target = [], []\n",
    "    for batch in testloader:\n",
    "        features = batch[\"features\"].to(device)\n",
    "        phones = batch[\"phones\"].to(device)\n",
    "        phone_labels = batch[\"phone_scores\"].to(device)\n",
    "        \n",
    "        utterance_preds, phone_preds, word_preds = gopt_model(x=features.float(), phn=phones.long())\n",
    "        \n",
    "        phone_preds = phone_preds.detach().cpu()\n",
    "        phone_labels = phone_labels.detach().cpu()\n",
    "        \n",
    "        A_phn.append(phone_preds[:, :, 0])\n",
    "        A_phn_target.append(phone_labels)\n",
    "        \n",
    "    A_phn, A_phn_target  = torch.vstack(A_phn), torch.vstack(A_phn_target)\n",
    "    phn_mse, phn_corr = valid_phn(A_phn, A_phn_target)\n",
    "    print(f\"### Validation result: MSE={phn_mse} PCC={phn_corr}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
