{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /data/codes/prep_ps_pykaldi\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import json\n",
    "from pandarallel import pandarallel\n",
    "import random\n",
    "import re\n",
    "\n",
    "pandarallel.initialize(nb_workers=10, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = \"/data/audio_data/prep_submission_audio/12\"\n",
    "metadata_path=\"/data/audio_data/pronunciation_scoring_result/dataset/info_out_domain_long_sentence_testset.csv\"\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "metadata.dropna(inplace=True)\n",
    "metadata.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_audio_is_exist(audio_id):\n",
    "    abs_path = os.path.join(audio_dir, f'{audio_id}.wav')\n",
    "    if os.path.exists(abs_path):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "metadata[\"is_exist\"] =  metadata.id.parallel_apply(check_audio_is_exist)\n",
    "print(metadata.shape)\n",
    "metadata = metadata[metadata[\"is_exist\"] == True]\n",
    "print(metadata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decision(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        raw_sample = json.load(f)\n",
    "\n",
    "    if \"api_version\" not in raw_sample:\n",
    "        return None\n",
    "            \n",
    "    utterance = raw_sample[\"utterance\"][0]    \n",
    "    decisions = []\n",
    "\n",
    "    for word in utterance[\"words\"]:\n",
    "        decisions.append(word[\"decision\"])\n",
    "\n",
    "    return decisions\n",
    "    \n",
    "json_dir = \"/data/audio_data/pronunciation_scoring_result/marking_data/12\"\n",
    "decisions = metadata.id.parallel_apply(lambda x: get_decision(os.path.join(json_dir, f'{x}.json')))\n",
    "decisions = decisions.explode()\n",
    "decisions.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decision(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        raw_sample = json.load(f)\n",
    "\n",
    "    if \"api_version\" not in raw_sample:\n",
    "        return None\n",
    "            \n",
    "    utterance = raw_sample[\"utterance\"][0]    \n",
    "    decisions = []\n",
    "\n",
    "    for word in utterance[\"words\"]:\n",
    "        for phoneme in word[\"phonemes\"]:\n",
    "            decisions.append(phoneme[\"decision\"])\n",
    "\n",
    "    return decisions\n",
    "    \n",
    "json_dir = \"/data/audio_data/pronunciation_scoring_result/marking_data/12\"\n",
    "decisions = metadata.id.parallel_apply(lambda x: get_decision(os.path.join(json_dir, f'{x}.json')))\n",
    "decisions = decisions.explode()\n",
    "decisions.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phoneme_error(json_path):\n",
    "    try:\n",
    "        with open(json_path, \"r\") as f:\n",
    "            raw_sample = json.load(f)\n",
    "\n",
    "        if \"api_version\" not in raw_sample:\n",
    "            return None\n",
    "                \n",
    "        utterance = raw_sample[\"utterance\"][0]    \n",
    "        phoneme_errors = []\n",
    "\n",
    "        for word in utterance[\"words\"]:\n",
    "            for phoneme in word[\"phonemes\"]:\n",
    "                if phoneme[\"phoneme_error\"] == \"normal\":\n",
    "                    phoneme_errors.append(phoneme[\"phoneme_error\"])\n",
    "                elif \"-\" in phoneme[\"phoneme_error\"]:\n",
    "                    phoneme_errors.append(\"other\")\n",
    "                else:\n",
    "                    phoneme_errors.append(phoneme[\"phoneme_error\"])\n",
    "                    \n",
    "        return phoneme_errors\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "json_dir = \"/data/audio_data/pronunciation_scoring_result/marking_data/12\"\n",
    "phoneme_errors = metadata.id.parallel_apply(lambda x: get_phoneme_error(os.path.join(json_dir, f'{x}.json')))\n",
    "phoneme_errors = phoneme_errors.explode()\n",
    "phoneme_errors.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_phoneme(phoneme):\n",
    "    if phoneme[\"phoneme_error_arpabet\"] != \"normal\":\n",
    "        trans = phoneme[\"phoneme_error_arpabet\"].split(\" - \")[-1]\n",
    "        if len(trans.split(\" \")) >= 2:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def extract_user_trans(phoneme):\n",
    "    valid = True\n",
    "    if phoneme[\"phoneme_error_arpabet\"] != \"normal\":\n",
    "        arpa, trans = phoneme[\"phoneme_error_arpabet\"].split(\" - \")\n",
    "        \n",
    "        try:\n",
    "            assert arpa == phoneme[\"trans_arpabet\"]\n",
    "        except:\n",
    "            valid = False\n",
    "            # print(phoneme[\"phoneme_error_arpabet\"], phoneme[\"trans_arpabet\"])\n",
    "    else:\n",
    "        arpa, trans = phoneme[\"trans_arpabet\"], phoneme[\"trans_arpabet\"]\n",
    "\n",
    "    return arpa, trans, valid\n",
    "        \n",
    "            \n",
    "def parse_metadata_data(json_path):\n",
    "    # try:\n",
    "        with open(json_path, \"r\") as f:\n",
    "            raw_sample = json.load(f)\n",
    "\n",
    "        if \"api_version\" not in raw_sample:\n",
    "            return None\n",
    "        \n",
    "        assert len(raw_sample[\"utterance\"]) == 1\n",
    "        \n",
    "         \n",
    "        words, phonemes = [], []\n",
    "        utterance = raw_sample[\"utterance\"][0] \n",
    "        for word in utterance[\"words\"]:\n",
    "                        \n",
    "            _phonemes, _trans_phonemes = [], []\n",
    "            for phoneme in word[\"phonemes\"]:\n",
    "                if not is_valid_phoneme(phoneme):\n",
    "                    return None\n",
    "            \n",
    "                arpa, trans, valid = extract_user_trans(phoneme)\n",
    "                if valid == False:\n",
    "                    return None\n",
    "                text = phoneme[\"text\"]\n",
    "                # score = phone_decision_to_score[phoneme[\"decision\"]]\n",
    "                score = phoneme[\"nativeness_score\"]\n",
    "\n",
    "                _phoneme = {\n",
    "                    \"text\": text,\n",
    "                    \"trans\": trans,\n",
    "                    \"arpa\": arpa,\n",
    "                    \"score\": score,\n",
    "                }\n",
    "                _phonemes.append(_phoneme)\n",
    "                _trans_phonemes.append(trans)\n",
    "            phonemes.append(_phonemes)\n",
    "\n",
    "            text = word[\"text\"]\n",
    "            arpa = word[\"trans_arpabet\"]\n",
    "            trans = \" \".join(_trans_phonemes).replace(\"SCHWA\", \"AH\")\n",
    "            # score = word_decision_to_score[word[\"decision\"]]\n",
    "            score = word[\"nativeness_score\"]\n",
    "\n",
    "            word = {\n",
    "                \"text\": text,\n",
    "                \"arpa\": arpa,\n",
    "                \"trans\": trans,\n",
    "                \"score\": score\n",
    "            }\n",
    "\n",
    "            words.append(word)\n",
    "                \n",
    "        metadata = {\n",
    "            \"words\": words,\n",
    "            \"phonemes\": phonemes,\n",
    "            \"utterance\": utterance[\"nativeness_score\"]\n",
    "        }\n",
    "\n",
    "        return json.dumps(metadata, ensure_ascii=False)\n",
    "    # except:\n",
    "    #     return None\n",
    "    \n",
    "json_dir = \"/data/audio_data/pronunciation_scoring_result/marking_data/12\"\n",
    "metadata[\"score\"] = metadata.id.parallel_apply(lambda x: parse_metadata_data(os.path.join(json_dir, f'{x}.json')))\n",
    "\n",
    "print(metadata.shape)\n",
    "metadata = metadata[metadata[\"score\"].notna()]\n",
    "print(metadata.shape)\n",
    "\n",
    "# metadata.head(100000).id.apply(lambda x: parse_metadata_data(os.path.join(json_dir, f'{x}.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phone_score(score):\n",
    "    phone_scores = []\n",
    "    score = json.loads(score)\n",
    "    for words in score[\"phonemes\"]:\n",
    "        for phoneme in words:\n",
    "            phone_scores.append(phoneme[\"score\"])\n",
    "\n",
    "    return phone_scores\n",
    "\n",
    "tmp = metadata.score.apply(lambda x: get_phone_score(x))\n",
    "tmp = tmp.explode()\n",
    "tmp.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_score(score):\n",
    "    word_scores = []\n",
    "    score = json.loads(score)\n",
    "    for words in score[\"words\"]:\n",
    "        word_scores.append(words[\"score\"])\n",
    "\n",
    "    return word_scores\n",
    "\n",
    "tmp = metadata.score.parallel_apply(lambda x: get_word_score(x))\n",
    "tmp = tmp.explode()\n",
    "tmp.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_score(score):\n",
    "    score = json.loads(score)[\"utterance\"]\n",
    "\n",
    "    return score\n",
    "\n",
    "tmp = metadata.score.parallel_apply(lambda x: get_sent_score(x))\n",
    "tmp.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(\"[\\,\\.\\;\\:\\!\\?]\", \" \", text)\n",
    "    text = text.upper()\n",
    "    text = text.strip()\n",
    "    return text\n",
    "metadata[\"question_content\"] = metadata.question_content.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexicon_path = \"resources/lexicon.txt\"\n",
    "# lexicon = pd.read_csv(lexicon_path, sep=\"\\t\", names=[\"word\", \"arpa\"])\n",
    "# lexicon.head()\n",
    "\n",
    "# vocab = {}\n",
    "# for index in lexicon.index:\n",
    "#     word = lexicon[\"word\"][index]\n",
    "#     arpa = lexicon[\"arpa\"][index]\n",
    "\n",
    "#     if word not in vocab:\n",
    "#         vocab[word] = [arpa, ]\n",
    "#     else:\n",
    "#         vocab[word].append(arpa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_path = \"resources/lexicon.txt\"\n",
    "vocab = {}\n",
    "with open(lexicon_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    lines.reverse()\n",
    "    lines = [line.strip().split() for line in lines]\n",
    "    lines = [[line[0], \" \".join(line[1:])] for line in lines]\n",
    "    \n",
    "    for word, phoneme in lines:\n",
    "        vocab[word] = phoneme\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(text, words):\n",
    "    words = json.loads(words)[\"words\"]\n",
    "    if len(text.split()) != len(words):\n",
    "        return False\n",
    "    for word, phoneme in zip(text.upper().split(), words):\n",
    "        if word not in vocab:\n",
    "            return False\n",
    "        if phoneme[\"arpa\"] not in vocab[word]:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "metadata[\"is_selected\"] = metadata.apply(lambda x: filter_data(x[\"question_content\"], x[\"score\"]), axis=1)\n",
    "print(metadata.shape)\n",
    "metadata = metadata[metadata[\"is_selected\"]==True].reset_index()\n",
    "print(metadata[metadata[\"is_selected\"]==True].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata[[\"user_id\", \"id\", \"question_content\", \"score\", \"question_id\"]].to_csv(\"/data/codes/prep_ps_pykaldi/prep_data/raw/info_out_domain_short_sentence_testset.csv\")\n",
    "metadata[[\"user_id\", \"id\", \"question_content\", \"score\", \"question_id\"]].to_csv(\"/data/codes/prep_ps_pykaldi/prep_data/raw/info_out_domain_long_sentence_testset_old.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
