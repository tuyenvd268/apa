{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /data/codes/prep_ps_pykaldi/\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_score_to_color(score, YELLOW_GREEN=85/50, RED_YELLOW=30/50):\n",
    "    LABEL2ID = {\"GREEN\": 0, \"YELLOW\": 1, \"RED\":2}\n",
    "    red_index = score < RED_YELLOW\n",
    "    yellow_index = ((score >= RED_YELLOW).int() & (score < YELLOW_GREEN).int()).bool()\n",
    "    green_index = score >= YELLOW_GREEN\n",
    "\n",
    "    score[red_index] = LABEL2ID[\"RED\"]\n",
    "    score[yellow_index] = LABEL2ID[\"YELLOW\"]\n",
    "    score[green_index] = LABEL2ID[\"GREEN\"]\n",
    "\n",
    "    return score\n",
    "\n",
    "def load_data(data_dir):\n",
    "    phone_ids = np.load(f'{data_dir}/phone_ids.npy')\n",
    "    phone_scores = np.load(f'{data_dir}/phone_scores.npy')\n",
    "    durations = np.load(f'{data_dir}/duration.npy')\n",
    "    gops = np.load(f'{data_dir}/gop.npy')\n",
    "    wavlm_features = np.load(f'{data_dir}/wavlm_features.npy')\n",
    "\n",
    "    return phone_ids, phone_scores, durations, gops, wavlm_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class PrepDataset(Dataset):\n",
    "    def __init__(self, phone_ids, phone_scores, durations, gops, wavlm_features):\n",
    "        self.phone_ids = phone_ids\n",
    "        self.phone_scores = phone_scores\n",
    "        self.gops = gops\n",
    "        self.durations = durations\n",
    "        self.wavlm_features = wavlm_features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.phone_ids.shape[0]\n",
    "    \n",
    "    def parse_data(self, phone_ids, phone_scores, gops, durations, wavlm_features):\n",
    "        phone_ids = torch.tensor(phone_ids)\n",
    "        durations = torch.tensor(durations)\n",
    "        gops = torch.tensor(gops)\n",
    "        phone_scores = torch.tensor(phone_scores).float().clone()\n",
    "        wavlm_features = torch.tensor(wavlm_features)\n",
    "\n",
    "        phone_scores[phone_scores != -1] /= 50\n",
    "\n",
    "        features = torch.concat([gops, durations.unsqueeze(-1), wavlm_features], dim=-1)        \n",
    "        return {\n",
    "            \"features\": features,\n",
    "            \"phone_ids\": phone_ids,\n",
    "            \"phone_scores\":phone_scores\n",
    "        }\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        phone_ids = self.phone_ids[index]\n",
    "        phone_scores = self.phone_scores[index]\n",
    "        gops = self.gops[index]\n",
    "        durations = self.durations[index]\n",
    "        wavlm_features = self.wavlm_features[index]\n",
    "\n",
    "        return self.parse_data(\n",
    "            phone_ids=phone_ids,\n",
    "            phone_scores=phone_scores,\n",
    "            gops=gops,\n",
    "            durations=durations,\n",
    "            wavlm_features=wavlm_features\n",
    "        )\n",
    "\n",
    "data_dir = \"/data/codes/prep_ps_pykaldi/exp/sm/in_short\"\n",
    "\n",
    "phone_ids, phone_scores, durations, gops, wavlm_features = load_data(data_dir)\n",
    "dataset_v1 = PrepDataset(phone_ids, phone_scores, durations, gops, wavlm_features)\n",
    "dataloader = DataLoader(dataset_v1, batch_size=8)\n",
    "\n",
    "for batch in dataloader:\n",
    "    features = batch[\"features\"]\n",
    "    phone_ids = batch[\"phone_ids\"]\n",
    "    phone_scores = batch[\"phone_scores\"]\n",
    "    \n",
    "    print(features.shape)\n",
    "    print(phone_ids.shape)\n",
    "    print(phone_scores.shape)\n",
    "    break\n",
    "\n",
    "dataset_v1 = None\n",
    "dataloader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def get_sinusoid_encoding(n_position, d_hid):\n",
    "    def get_position_angle_vec(position):\n",
    "        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
    "\n",
    "    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "\n",
    "    return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n",
    "\n",
    "\n",
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    def norm_cdf(x):\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "        tensor.erfinv_()\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        \n",
    "        return tensor\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class GOPT(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, depth, input_dim=84, max_length=50, num_phone=40):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(dim=embed_dim, num_heads=num_heads) \n",
    "                for i in range(depth)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, max_length+1, self.embed_dim))\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "\n",
    "        self.in_proj = nn.Linear(self.input_dim, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim * 2, embed_dim)\n",
    "        self.mlp_head_phn = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))\n",
    "\n",
    "        self.mlp_head_word= nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))\n",
    "\n",
    "        self.num_phone = num_phone\n",
    "        self.phn_proj = nn.Linear(num_phone, embed_dim)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.mlp_head_utt = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))\n",
    "\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "\n",
    "    def forward(self, x, phn):\n",
    "        B = x.shape[0]\n",
    "        phn_one_hot = torch.nn.functional.one_hot(phn.long()+1, num_classes=self.num_phone).float()\n",
    "        phn_embed = self.phn_proj(phn_one_hot)\n",
    "\n",
    "        if self.embed_dim != self.input_dim:\n",
    "            x = self.in_proj(x)\n",
    "\n",
    "        x = torch.cat([x, phn_embed], dim=-1)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        cls_token = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = x + self.pos_embed[:,:x.shape[1],:]\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        u = self.mlp_head_utt(x[:, 0])\n",
    "        p = self.mlp_head_phn(x[:, 1:])\n",
    "        w = self.mlp_head_word(x[:, 1:])\n",
    "        return u, p, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "data_dir = \"/data/codes/prep_ps_pykaldi/exp/sm/train\"\n",
    "phone_ids, phone_scores, durations, gops, wavlm_features = load_data(data_dir)\n",
    "trainset = PrepDataset(\n",
    "    phone_ids, \n",
    "    phone_scores, \n",
    "    durations, \n",
    "    gops, \n",
    "    wavlm_features\n",
    ")\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=8, shuffle=True, drop_last=False)\n",
    "\n",
    "data_dir = \"/data/codes/prep_ps_pykaldi/exp/sm/in_short\"\n",
    "phone_ids, phone_scores, durations, gops, wavlm_features = load_data(data_dir)\n",
    "testset = PrepDataset(\n",
    "    phone_ids, \n",
    "    phone_scores, \n",
    "    durations, \n",
    "    gops, \n",
    "    wavlm_features\n",
    ")\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=8, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gopt_model = GOPT(\n",
    "    embed_dim=32, num_heads=1, \n",
    "    depth=3, input_dim=851, \n",
    "    max_length=128, num_phone=62).to(device)\n",
    "\n",
    "trainables = [p for p in gopt_model.parameters() if p.requires_grad]\n",
    "\n",
    "lr = 3e-4\n",
    "optimizer = torch.optim.Adam(\n",
    "    trainables, lr, weight_decay=5e-7, betas=(0.95, 0.999))\n",
    "\n",
    "scheduler = MultiStepLR(\n",
    "    optimizer, list(range(10, 100, 5)), gamma=0.5, last_epoch=-1)\n",
    "\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_phn(audio_output, target):\n",
    "    valid_token_pred = []\n",
    "    valid_token_target = []\n",
    "    # audio_output = audio_output.squeeze(2)\n",
    "    for i in range(audio_output.shape[0]):\n",
    "        for j in range(audio_output.shape[1]):\n",
    "            # only count valid tokens, not padded tokens (represented by negative values)\n",
    "            if target[i, j] >= 0:\n",
    "                valid_token_pred.append(audio_output[i, j])\n",
    "                valid_token_target.append(target[i, j])\n",
    "    valid_token_target = np.array(valid_token_target)\n",
    "    valid_token_pred = np.array(valid_token_pred)\n",
    "\n",
    "    valid_token_mse = np.mean((valid_token_target - valid_token_pred) ** 2)\n",
    "    valid_token_mae = np.mean(np.abs(valid_token_target - valid_token_pred))\n",
    "    corr = np.corrcoef(valid_token_pred, valid_token_target)[0, 1]\n",
    "    return valid_token_mse, valid_token_mae, corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "exp_dir = \"/data/codes/prep_ps_pykaldi/exp/preds\"\n",
    "ckpt_dir = \"/data/codes/prep_ps_pykaldi/exp/ckpts\"\n",
    "\n",
    "global_step = 0\n",
    "best_mse = 1e5\n",
    "for epoch in range(50):\n",
    "    gopt_model.train()\n",
    "    train_tqdm = tqdm(trainloader, \"Training\")\n",
    "\n",
    "    for batch in train_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        features = batch[\"features\"].to(device)\n",
    "        phone_ids = batch[\"phone_ids\"].to(device)\n",
    "        phone_labels = batch[\"phone_scores\"].to(device)\n",
    "\n",
    "        utterance_preds, phone_preds, word_preds = gopt_model(x=features.float(), phn=phone_ids.long())\n",
    "        \n",
    "        mask = phone_labels >=0\n",
    "        phone_preds = phone_preds.squeeze(2)\n",
    "        phone_preds = phone_preds * mask\n",
    "        phone_labels = phone_labels * mask\n",
    "        \n",
    "        loss_phn = loss_fn(phone_preds, phone_labels)\n",
    "        loss_phn = loss_phn * (mask.shape[0] * mask.shape[1]) / torch.sum(mask)\n",
    "        \n",
    "        loss_phn.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        global_step += 1\n",
    "        train_tqdm.set_postfix(loss_phn=loss_phn.item())\n",
    "    \n",
    "    A_phn, A_phn_target = [], []\n",
    "    for batch in testloader:\n",
    "        features = batch[\"features\"].to(device)\n",
    "        phone_ids = batch[\"phone_ids\"].to(device)\n",
    "        phone_labels = batch[\"phone_scores\"].to(device)\n",
    "        \n",
    "        utterance_preds, phone_preds, word_preds = gopt_model(x=features.float(), phn=phone_ids.long())\n",
    "        \n",
    "        phone_preds = phone_preds.detach().cpu()\n",
    "        phone_labels = phone_labels.detach().cpu()\n",
    "        \n",
    "        A_phn.append(phone_preds[:, :, 0])\n",
    "        A_phn_target.append(phone_labels)\n",
    "        \n",
    "    A_phn, A_phn_target  = torch.vstack(A_phn), torch.vstack(A_phn_target)\n",
    "\n",
    "    indices = A_phn_target != -1\n",
    "    _label = A_phn_target[indices].clone()\n",
    "    _pred = A_phn[indices].clone()\n",
    "\n",
    "    converted_pred = convert_score_to_color(_pred).view(-1)\n",
    "    converted_label = convert_score_to_color(_label).view(-1)\n",
    "\n",
    "    print(\"### F1 Score: \\n\", classification_report(y_true=converted_label, y_pred=converted_pred))\n",
    "\n",
    "    # valid_token_mse, valid_token_mae, corr\n",
    "    phn_mse, phn_mae, phn_corr = valid_phn(A_phn, A_phn_target)\n",
    "\n",
    "    if phn_mse < best_mse:\n",
    "        best_mse = phn_mse\n",
    "        print(\"### Saved predict and label\")\n",
    "        np.save(f'{exp_dir}/pred.npy', A_phn.numpy())\n",
    "        np.save(f'{exp_dir}/label.npy', A_phn_target.numpy())\n",
    "        \n",
    "    print(f\"### Validation result (epoch={epoch}): MSE={round(phn_mse, 4)} MAE={round(phn_mae, 4)} PCC={round(phn_corr, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training: 100%|██████████| 18740/18740 [00:54<00:00, 342.27it/s, loss_phn=0.232] \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.90      0.96      0.93    104138\n",
    "#          1.0       0.32      0.28      0.30     12537\n",
    "#          2.0       0.73      0.42      0.53     12449\n",
    "\n",
    "#     accuracy                           0.84    129124\n",
    "#    macro avg       0.65      0.55      0.59    129124\n",
    "# weighted avg       0.83      0.84      0.83    129124\n",
    "\n",
    "# ### Saved predict and label\n",
    "# ### Validation result (epoch=0): MSE=0.18320000171661377 MAE=0.2574000060558319 PCC=0.7568\n",
    "# Training: 100%|██████████| 18740/18740 [00:54<00:00, 343.74it/s, loss_phn=0.137] \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.91      0.96      0.93    104138\n",
    "#          1.0       0.34      0.32      0.33     12537\n",
    "#          2.0       0.73      0.46      0.57     12449\n",
    "\n",
    "#     accuracy                           0.85    129124\n",
    "#    macro avg       0.66      0.58      0.61    129124\n",
    "# weighted avg       0.84      0.85      0.84    129124\n",
    "\n",
    "# ### Saved predict and label\n",
    "# ### Validation result (epoch=1): MSE=0.17579999566078186 MAE=0.26809999346733093 PCC=0.7729\n",
    "# Training: 100%|██████████| 18740/18740 [00:54<00:00, 342.07it/s, loss_phn=0.175] \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.95      0.93    104138\n",
    "#          1.0       0.34      0.34      0.34     12537\n",
    "#          2.0       0.72      0.50      0.59     12449\n",
    "\n",
    "#     accuracy                           0.85    129124\n",
    "#    macro avg       0.66      0.60      0.62    129124\n",
    "# weighted avg       0.84      0.85      0.84    129124\n",
    "\n",
    "# ### Saved predict and label\n",
    "# ### Validation result (epoch=2): MSE=0.1720999926328659 MAE=0.25369998812675476 PCC=0.7801\n",
    "# Training: 100%|██████████| 18740/18740 [00:54<00:00, 345.56it/s, loss_phn=0.168] \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.95      0.93    104138\n",
    "#          1.0       0.34      0.32      0.33     12537\n",
    "#          2.0       0.68      0.57      0.62     12449\n",
    "\n",
    "#     accuracy                           0.85    129124\n",
    "#    macro avg       0.65      0.61      0.63    129124\n",
    "# weighted avg       0.84      0.85      0.84    129124\n",
    "\n",
    "# ### Validation result (epoch=3): MSE=0.17890000343322754 MAE=0.25999999046325684 PCC=0.7795\n",
    "# Training: 100%|██████████| 18740/18740 [00:54<00:00, 344.52it/s, loss_phn=0.0753]\n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.95      0.93    104138\n",
    "#          1.0       0.35      0.29      0.32     12537\n",
    "#          2.0       0.68      0.60      0.64     12449\n",
    "\n",
    "#     accuracy                           0.85    129124\n",
    "#    macro avg       0.65      0.61      0.63    129124\n",
    "# weighted avg       0.84      0.85      0.85    129124\n",
    "\n",
    "# ### Saved predict and label\n",
    "# ### Validation result (epoch=4): MSE=0.1695999950170517 MAE=0.2418999969959259 PCC=0.7867\n",
    "# Training: 100%|██████████| 18740/18740 [00:54<00:00, 344.80it/s, loss_phn=0.119] \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.91      0.96      0.94    104138\n",
    "#          1.0       0.34      0.33      0.34     12537\n",
    "#          2.0       0.75      0.47      0.58     12449\n",
    "\n",
    "#     accuracy                           0.85    129124\n",
    "#    macro avg       0.67      0.59      0.62    129124\n",
    "# weighted avg       0.84      0.85      0.84    129124\n",
    "\n",
    "# ### Saved predict and label\n",
    "# ### Validation result (epoch=5): MSE=0.16220000386238098 MAE=0.2500999867916107 PCC=0.7904\n",
    "# Training: 100%|██████████| 18740/18740 [00:54<00:00, 342.43it/s, loss_phn=0.0393]\n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.91      0.96      0.94    104138\n",
    "#          1.0       0.35      0.29      0.32     12537\n",
    "#          2.0       0.76      0.50      0.60     12449\n",
    "\n",
    "#     accuracy                           0.85    129124\n",
    "#    macro avg       0.67      0.58      0.62    129124\n",
    "# weighted avg       0.84      0.85      0.84    129124\n",
    "\n",
    "# ### Saved predict and label\n",
    "# ### Validation result (epoch=6): MSE=0.15809999406337738 MAE=0.2386000007390976 PCC=0.7935\n",
    "# Training: 100%|██████████| 18740/18740 [00:55<00:00, 339.46it/s, loss_phn=0.0975]\n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.96      0.94    104138\n",
    "#          1.0       0.35      0.30      0.32     12537\n",
    "#          2.0       0.72      0.54      0.62     12449\n",
    "\n",
    "#     accuracy                           0.85    129124\n",
    "#    macro avg       0.66      0.60      0.62    129124\n",
    "# weighted avg       0.84      0.85      0.85    129124\n",
    "\n",
    "# ### Validation result (epoch=7): MSE=0.16140000522136688 MAE=0.23399999737739563 PCC=0.793\n",
    "# Training: 100%|██████████| 18740/18740 [00:54<00:00, 341.84it/s, loss_phn=0.13]  \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.95      0.94    104138\n",
    "#          1.0       0.35      0.31      0.33     12537\n",
    "#          2.0       0.71      0.56      0.62     12449\n",
    "\n",
    "#     accuracy                           0.85    129124\n",
    "#    macro avg       0.66      0.61      0.63    129124\n",
    "# weighted avg       0.84      0.85      0.85    129124\n",
    "\n",
    "# ### Validation result (epoch=8): MSE=0.16449999809265137 MAE=0.23659999668598175 PCC=0.7911\n",
    "# Training: 100%|██████████| 18740/18740 [00:54<00:00, 341.18it/s, loss_phn=0.215]  \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.95      0.94    104138\n",
    "#          1.0       0.36      0.31      0.33     12537\n",
    "#          2.0       0.71      0.58      0.64     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.66      0.61      0.64    129124\n",
    "# weighted avg       0.84      0.86      0.85    129124\n",
    "\n",
    "# ### Validation result (epoch=9): MSE=0.16279999911785126 MAE=0.2371000051498413 PCC=0.7939\n",
    "# Training: 100%|██████████| 18740/18740 [00:57<00:00, 324.39it/s, loss_phn=0.12]  \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.96      0.94    104138\n",
    "#          1.0       0.36      0.30      0.33     12537\n",
    "#          2.0       0.72      0.55      0.63     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.67      0.61      0.63    129124\n",
    "# weighted avg       0.84      0.86      0.85    129124\n",
    "\n",
    "# ### Validation result (epoch=10): MSE=0.16130000352859497 MAE=0.22550000250339508 PCC=0.7938\n",
    "# Training: 100%|██████████| 18740/18740 [00:54<00:00, 345.07it/s, loss_phn=0.142] \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.91      0.96      0.94    104138\n",
    "#          1.0       0.36      0.29      0.32     12537\n",
    "#          2.0       0.75      0.53      0.62     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.67      0.59      0.62    129124\n",
    "# weighted avg       0.84      0.86      0.85    129124\n",
    "\n",
    "# ### Saved predict and label\n",
    "# ### Validation result (epoch=11): MSE=0.15600000321865082 MAE=0.2273000031709671 PCC=0.7976\n",
    "# Training: 100%|██████████| 18740/18740 [00:55<00:00, 339.19it/s, loss_phn=0.156] \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.95      0.94    104138\n",
    "#          1.0       0.36      0.34      0.35     12537\n",
    "#          2.0       0.72      0.57      0.64     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.67      0.62      0.64    129124\n",
    "# weighted avg       0.85      0.86      0.85    129124\n",
    "\n",
    "# ### Validation result (epoch=12): MSE=0.16580000519752502 MAE=0.25450000166893005 PCC=0.7952\n",
    "# Training: 100%|██████████| 18740/18740 [00:58<00:00, 323.06it/s, loss_phn=0.163]  \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.91      0.96      0.94    104138\n",
    "#          1.0       0.36      0.29      0.32     12537\n",
    "#          2.0       0.73      0.56      0.63     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.67      0.60      0.63    129124\n",
    "# weighted avg       0.84      0.86      0.85    129124\n",
    "\n",
    "# ### Validation result (epoch=13): MSE=0.15639999508857727 MAE=0.23199999332427979 PCC=0.7995\n",
    "# Training: 100%|██████████| 18740/18740 [00:54<00:00, 341.36it/s, loss_phn=0.163]  \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.96      0.94    104138\n",
    "#          1.0       0.37      0.31      0.34     12537\n",
    "#          2.0       0.73      0.56      0.63     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.67      0.61      0.64    129124\n",
    "# weighted avg       0.85      0.86      0.85    129124\n",
    "\n",
    "# ### Validation result (epoch=14): MSE=0.15610000491142273 MAE=0.2280000001192093 PCC=0.8007\n",
    "# Training: 100%|██████████| 18740/18740 [00:54<00:00, 345.71it/s, loss_phn=0.19]  \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.91      0.96      0.94    104138\n",
    "#          1.0       0.37      0.29      0.32     12537\n",
    "#          2.0       0.75      0.54      0.63     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.68      0.60      0.63    129124\n",
    "# weighted avg       0.84      0.86      0.85    129124\n",
    "\n",
    "# ### Saved predict and label\n",
    "# ### Validation result (epoch=15): MSE=0.15489999949932098 MAE=0.2207999974489212 PCC=0.7996\n",
    "# Training: 100%|██████████| 18740/18740 [00:55<00:00, 338.56it/s, loss_phn=0.179] \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.95      0.94    104138\n",
    "#          1.0       0.36      0.31      0.34     12537\n",
    "#          2.0       0.70      0.61      0.65     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.66      0.62      0.64    129124\n",
    "# weighted avg       0.85      0.86      0.85    129124\n",
    "\n",
    "# ### Validation result (epoch=16): MSE=0.16220000386238098 MAE=0.23729999363422394 PCC=0.7977\n",
    "# Training: 100%|██████████| 18740/18740 [00:55<00:00, 340.35it/s, loss_phn=0.0687] \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.96      0.94    104138\n",
    "#          1.0       0.36      0.31      0.33     12537\n",
    "#          2.0       0.74      0.56      0.64     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.67      0.61      0.64    129124\n",
    "# weighted avg       0.85      0.86      0.85    129124\n",
    "\n",
    "# ### Saved predict and label\n",
    "# ### Validation result (epoch=17): MSE=0.15449999272823334 MAE=0.23019999265670776 PCC=0.8019\n",
    "# Training: 100%|██████████| 18740/18740 [00:54<00:00, 343.38it/s, loss_phn=0.211] \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.91      0.96      0.94    104138\n",
    "#          1.0       0.37      0.29      0.33     12537\n",
    "#          2.0       0.74      0.55      0.63     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.67      0.60      0.63    129124\n",
    "# weighted avg       0.84      0.86      0.85    129124\n",
    "\n",
    "# ### Saved predict and label\n",
    "# ### Validation result (epoch=18): MSE=0.1527000069618225 MAE=0.22200000286102295 PCC=0.8025\n",
    "# Training: 100%|██████████| 18740/18740 [00:54<00:00, 345.40it/s, loss_phn=0.156]  \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.95      0.94    104138\n",
    "#          1.0       0.37      0.33      0.35     12537\n",
    "#          2.0       0.72      0.58      0.65     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.67      0.62      0.64    129124\n",
    "# weighted avg       0.85      0.86      0.85    129124\n",
    "\n",
    "# ### Validation result (epoch=19): MSE=0.15940000116825104 MAE=0.24060000479221344 PCC=0.8\n",
    "# Training: 100%|██████████| 18740/18740 [00:55<00:00, 338.93it/s, loss_phn=0.133] \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.96      0.94    104138\n",
    "#          1.0       0.37      0.30      0.33     12537\n",
    "#          2.0       0.73      0.56      0.64     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.67      0.61      0.64    129124\n",
    "# weighted avg       0.84      0.86      0.85    129124\n",
    "\n",
    "# ### Validation result (epoch=20): MSE=0.15620000660419464 MAE=0.23100000619888306 PCC=0.7998\n",
    "# Training: 100%|██████████| 18740/18740 [00:55<00:00, 339.05it/s, loss_phn=0.337] \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.96      0.94    104138\n",
    "#          1.0       0.37      0.30      0.33     12537\n",
    "#          2.0       0.71      0.60      0.65     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.67      0.62      0.64    129124\n",
    "# weighted avg       0.85      0.86      0.85    129124\n",
    "\n",
    "# ### Validation result (epoch=21): MSE=0.156700000166893 MAE=0.22679999470710754 PCC=0.8013\n",
    "# Training: 100%|██████████| 18740/18740 [00:56<00:00, 331.85it/s, loss_phn=0.197] \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.95      0.94    104138\n",
    "#          1.0       0.36      0.30      0.33     12537\n",
    "#          2.0       0.71      0.58      0.64     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.66      0.61      0.63    129124\n",
    "# weighted avg       0.84      0.86      0.85    129124\n",
    "\n",
    "# ### Validation result (epoch=22): MSE=0.1615000069141388 MAE=0.2387000024318695 PCC=0.7966\n",
    "# Training: 100%|██████████| 18740/18740 [00:51<00:00, 362.69it/s, loss_phn=0.355] \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.96      0.94    104138\n",
    "#          1.0       0.37      0.30      0.33     12537\n",
    "#          2.0       0.72      0.59      0.65     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.67      0.62      0.64    129124\n",
    "# weighted avg       0.85      0.86      0.85    129124\n",
    "\n",
    "# ### Validation result (epoch=23): MSE=0.1565999984741211 MAE=0.22200000286102295 PCC=0.801\n",
    "# Training: 100%|██████████| 18740/18740 [00:50<00:00, 371.22it/s, loss_phn=0.236]  \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.91      0.96      0.94    104138\n",
    "#          1.0       0.37      0.30      0.33     12537\n",
    "#          2.0       0.73      0.57      0.64     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.67      0.61      0.63    129124\n",
    "# weighted avg       0.84      0.86      0.85    129124\n",
    "\n",
    "# ### Validation result (epoch=24): MSE=0.15569999814033508 MAE=0.2321999967098236 PCC=0.8001\n",
    "# Training: 100%|██████████| 18740/18740 [00:50<00:00, 369.94it/s, loss_phn=0.055]  \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.96      0.94    104138\n",
    "#          1.0       0.36      0.31      0.34     12537\n",
    "#          2.0       0.71      0.57      0.63     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.66      0.61      0.63    129124\n",
    "# weighted avg       0.84      0.86      0.85    129124\n",
    "\n",
    "# ### Validation result (epoch=25): MSE=0.16009999811649323 MAE=0.23090000450611115 PCC=0.7975\n",
    "# Training: 100%|██████████| 18740/18740 [00:51<00:00, 361.52it/s, loss_phn=0.143]  \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.96      0.94    104138\n",
    "#          1.0       0.37      0.28      0.32     12537\n",
    "#          2.0       0.71      0.59      0.65     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.67      0.61      0.63    129124\n",
    "# weighted avg       0.84      0.86      0.85    129124\n",
    "\n",
    "# ### Validation result (epoch=26): MSE=0.15809999406337738 MAE=0.22050000727176666 PCC=0.7996\n",
    "# Training: 100%|██████████| 18740/18740 [00:50<00:00, 368.01it/s, loss_phn=0.173] \n",
    "# ### F1 Score: \n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.95      0.94    104138\n",
    "#          1.0       0.36      0.29      0.32     12537\n",
    "#          2.0       0.67      0.64      0.66     12449\n",
    "\n",
    "#     accuracy                           0.86    129124\n",
    "#    macro avg       0.65      0.63      0.64    129124\n",
    "# weighted avg       0.84      0.86      0.85    129124\n",
    "\n",
    "# ### Validation result (epoch=27): MSE=0.1648000031709671 MAE=0.2402999997138977 PCC=0.7975\n",
    "# Training:  40%|████      | 7526/18740 [00:20<00:30, 363.49it/s, loss_phn=0.16]   \n",
    "# ---------------------------------------------------------------------------\n",
    "# KeyboardInterrupt                         Traceback (most recent call last)\n",
    "# /data/codes/prep_ps_pykaldi/notebooks/3mh-gopt-dur-wavlmb_main.ipynb Cell 8 line 1\n",
    "#      11 for batch in train_tqdm:\n",
    "#      12     optimizer.zero_grad()\n",
    "# ---> 14     features = batch[\"features\"].to(device)\n",
    "#      15     phone_ids = batch[\"phone_ids\"].to(device)\n",
    "#      16     phone_labels = batch[\"phone_scores\"].to(device)\n",
    "\n",
    "# KeyboardInterrupt: \n",
    "# /data/codes/prep_ps_pykaldi\n",
    "# ---------------------------------------------------------------------------\n",
    "# FileNotFoundError                         Traceback (most recent call last)\n",
    "# /data/codes/prep_ps_pykaldi/notebooks/3mh-gopt-dur-wavlmb_main.ipynb Cell 3 line 4\n",
    "#      38         return self.parse_data(\n",
    "#      39             phone_ids=phone_ids,\n",
    "#      40             phone_scores=phone_scores,\n",
    "#    (...)\n",
    "#      43             wavlm_features=wavlm_features\n",
    "#      44         )\n",
    "#      46 data_dir = \"/data/codes/prep_ps_pykaldi/exp/sm/test\"\n",
    "# ---> 48 phone_ids, phone_scores, durations, gops, wavlm_features = load_data(data_dir)\n",
    "#      49 dataset_v1 = PrepDataset(phone_ids, phone_scores, durations, gops, wavlm_features)\n",
    "#      50 dataloader = DataLoader(dataset_v1, batch_size=8)\n",
    "\n",
    "# /data/codes/prep_ps_pykaldi/notebooks/3mh-gopt-dur-wavlmb_main.ipynb Cell 3 line 1\n",
    "#      13 def load_data(data_dir):\n",
    "# ---> 14     phone_ids = np.load(f'{data_dir}/phone_ids.npy')\n",
    "#      15     phone_scores = np.load(f'{data_dir}/phone_scores.npy')\n",
    "#      16     durations = np.load(f'{data_dir}/duration.npy')\n",
    "\n",
    "# File ~/miniconda3/envs/ps/lib/python3.8/site-packages/numpy/lib/npyio.py:390, in load(file, mmap_mode, allow_pickle, fix_imports, encoding)\n",
    "#     388     own_fid = False\n",
    "#     389 else:\n",
    "# --> 390     fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n",
    "#     391     own_fid = True\n",
    "#     393 # Code to distinguish from NumPy binary files and pickles.\n",
    "\n",
    "# FileNotFoundError: [Errno 2] No such file or directory: '/data/codes/prep_ps_pykaldi/exp/sm/test/phone_ids.npy'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
